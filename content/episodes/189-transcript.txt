Siamo noi che nonostante il deploy fallito, la CIA rossa, il business incazzato, ci troviamo al GITBAR e davanti a una birra tutto ci sembra un po' meno grave. Bene e benvenuti su GITBAR, nuova settimana e nuovo episodio qua nel nostro bar degli sviluppatori. Bar degli sviluppatori che oggi vede un mio compare, amico di Knuze, Andrea, ciao! Ciao Mauro, grande! Quanto tempo! Sempre un piacere venire qui a Balcone. È bello bere insieme, suona malissimo questa cosa, però va bene lo stesso. No, no, va bene uguale. Mi fa super piacere fare questa puntata con te, anche perché oggi ci hai portato una però prima di annunciarvela abbiamo come nostro solito il dovere di ricordarvi i nostri contatti, info@gitbar.it @brainrepo su Twitter e il grande gruppone, il gruppone che ormai, mazza, 1531 membri, di cui 230 online in questo momento mentre stiamo registrando, e bomba, basta aprire il client telegram preferito e cercare Gitbar, imboccare e siamo qui. Esatto, è la nostra casa, il posto dove realmente ci incontriamo per fare i follow up e parlare delle cose più disparate, dai bisticci tra YAML, XML e JSON, alle infinite discussioni sulla RAL, a Irant su Java 8 insomma ce n'è per tutti i gusti per cui se non l'avete ancora fatto iscrivetevi mi preme ricordarvi anche che abbiamo il nostro giovane, giovanissimo canale YouTube, canale YouTube che sta partendo, siamo appena partiti e che ha bisogno del vostro like e sul campanaccio che suoniate il campanaccio per rimanere aggiornati su tutte le novità. Detto questo, rapidissimamente vi ricordo anche che abbiamo uno store, abbiamo fatto la nostra maglietta di video terminalista metal meccanico, quindi se volete prenderla c'è lo store delle maglie di Geekbar, vedo che l'ospite già ride, in realtà si è un pezzo d'arte, mi piacerebbe definirla così. Ehm, e bom, penso che abbiamo detto tutto, allora a questo punto possono iniziare i rant del vecchio sviluppatore acciaccato Ragazzi, io non so come voi facciate, ma ci sono dei giorni che ho un male alla mano sinistra, alle dita, che è una cosa micidiale Perché fai troppo "command tab, command tab, command tab"? ma non lo so Credimi io ho provato con la split keyboard ho provato con la trackball Tutte le ho provate in realtà c'ho questi crampi alla mano che non vogliono passare o il terrore di dovermi operare alla al tunnel carpale quella quella cosa la carpale Ok, quindi se avete qualche suggerimento, tipo vecchi anziani che si scambiano le medicine, ecco potete usare il gruppo Telegram per suggerirmi delle possibili soluzioni, tra l'altro dovremo organizzare una puntata sul well-being, su come ottimizzare la qualità della nostra vita. "Bro questo è tutto un altro topic, allora io direi che possiamo presentare il nostro ospite. Andrea piccola sigletta e poi parti". Allora, come presentare l'ospite di stasera? Prima tutto c'è da dire che io conosco l'ospite di stasera da tipo 7-8 anni e mi ricordo benissimo questa fotografia, eravamo alla cena post meetup di RomaJS eravamo io, lui e Matteo Manchi. Ciao Matteo, cogliamo l'occasione di salutarlo. Stavamo parlando di Docker, come utilizzare immagini Docker in produzione, come DeployArm, Docker Swarm o direttamente Docker Compose sulla macchina in produzione, eccetera eccetera. E c'è questo ricordo, ci presentiamo e da quel momento io puntualmente sbaglio il suo nome e lui mi oglia per questo. Però ho una scusante per questo, perché in quegli anni, in quel periodo della mia vita, stavo totalmente in fissa goggomorra e quindi con Pietro Savastano. E io da allora, ogni volta che lo vedo, lo cerco, che gli voglio scrivere o gli sto scrivendo, o lo incontro, lo saluto, la prima cosa che dico è "Oh ciao Piero!" "Piero, cazzo, Piero!" E quindi stasera abbiamo Piero Savastano, on stage. Ciao ragazzi, grazie. Io spero che tu non interpreti la nostra professione, sai, stare davanti al computer con un 41 bis perché un po' si assomiglia no? Sì, diciamo stai senza pensieri. Sta senza pensieri. Scherzi a parte, intanto sono super felice di averti qua su Gitbar anche perché c'ho un pacco di domande da farti sulla tua nuova creatura ma anche su come gestisci la tua presenza. La chiamo presenza social ma non è necessariamente social, chiamiamola presenza mediale. Io ormai a livello nazionale sei diventato uno dei personaggi del panorama. Dell'ecosistema LLM, del language modern. Ma io direi non solo in quel filone verticale perché hai fatto tantissime attività anche con CodeMotion, io ricordo che sei stato co-presentatore o qualcosa del genere qualche anno fa, insomma hai scritto tanto, hai fatto YouTube e quindi la presenza mediale è proprio, cioè la si sente, il tuo viso lo si riconosce subito. Mi chiedo allora come ci si costruisce una presenza pubblica in questo modo riuscendo però a sua volta a veicolare contenuti di qualità come quelli che fondamentalmente stai veicolando, cioè io vorrei essere più presente nei social, ma talvolta mi dico "ma non posso essere presente a dire cazzate" e a pagina due le cose interessanti da dire prima o poi finiscono, quindi come gestisci questo equilibrio tra la presenza e la profondità dei contenuti? Un compromesso continuo che è difficile un po' da mantenere, secondo me un requisito è essere creativi, non aver paura di esprimersi, non farsi condizionare troppo da obblighi di calendario quando sei stanco, ritagliarsi del tempo, sperimentare. Un compromesso che ho trovato è utilizzare i social sia come canale espressivo, proprio puramente creativo, ho fatto degli esperimenti un po' malati interpretando un messicano che a un certo punto ho abbandonato, però in generale penso che l'aspetto creativo sia la parte essenziale, poi è anche un ottimo canale poi di marketing se vogliamo per la professione, per tutte le attività più tecnico-scientifiche. A questo punto la domanda sulla quantità di lavoro viene immediatamente successiva cioè nel day by day quanto c'è di creazione contenuti, pubblicazione, stare dietro alle community? Ok, allora se il contenuto deve essere di alta qualità se vai su youtube per macinare più numeri c'è da scrivere il video, girarlo, montarlo, la disseminazione poi del video è un'attività a parte che molti trascurano. Se invece si va su social che sono un po' più immediati come TikTok uno basta che accende e parla. Quindi in base a quanto tempo ha a disposizione, per dire se ho un'ora al giorno TikTok, se ho due o tre giorni a disposizione magari penso a un video. Per quanto riguarda la community lei poi diventa un mostro otto teste perché come tu pensai, immagini, poi diventi un coordinatore oltre che il produttore dei contenuti e quindi hai delle relazioni, hai anche un aspetto proprio di gestione. Sì è vero, iniziano tutte le cose che non si vedono, no? I direct message. Io passo un sacco di tempo a rispondere ai direct, a parlare con le persone indirect che è una cosa che non emerge dalla presenza pubblica, perché quando entri in direct la relazione one to one non appare pubblicamente, però comunque assorbe una certa quantità di effort. Però a parte la questione mediatica, a questo punto io direi di andare a parlare della ciccia, perché Piero è... C'è una bella t-shirt sul petto no? Eh sì, guarda che bellezza. Dicevo, tra l'altro c'è lo store perché la voglio. Sì sì, ragazzi, ve la mando sicuramente anche senza store, però ve la facciamo avere. Io ho un certo feticismo per le magliette come potete vedere dai vari episodi. Dicevo, i contenuti occupano del tempo, ma io mi chiedo come fai poi in realtà, oltre al lavoro, oltre ai contenuti, anche realizzare progetti open source di una certa caratura come "Lo Stregato"? Da quando ho lanciato "Lo Stregato" rischio, in circa da un anno, di finire sotto un ponte. questa è la sintesi dell'ultimo anno per quanto riguarda dal punto di vista professionale mi sto prendendo dei grossi rischi fino a che non avevo il mantenimento del framework mi testeggiavo tra contenuti attività professionale, corsi di formazione abbastanza agevolmente, era arrivato ad un buon punto da quando ho cominciato con lo stregatto ha preso il sopravvento tutto il resto quindi a certo punto mi sono trovato a decidere che faccio cerco di mantenerlo, lo passo a qualcuno, la decisione che ho preso è stata rischio tutto, quindi diciamo che è da un anno che ci lavoro pro bono. Senza riuscire a beneficiare di consulenza per introdurti a utilizzare questo strumento open source in azienda o per casi d'uso particolari, cioè quindi ancora eh diciamo ti stai proprio concentrato stai evitando tutto questo per concentrarti sullo strumento per dare il massimo focus sulle sue evoluzioni. Sì, in effetti ti do ragione sul fatto che le opportunità emergono, crescono anche, sono anche di di un calibro maggiore rispetto a quelle del passato, però per scelta voglio fare il mann... maintainer, cioè mi sono proprio messo in testa di dare tutto quello che ho per portarlo più in alto possibile. Allora domanda proprio tran shan di quelle provocatrici alla github, ma si può vivere di open source dal tuo punto di vista? Secondo me sì con un po' di accorgimenti e anche un po' di colpi di scena, se vi va approfondiamo perché è una domanda che se ne parla poco secondo me. Chiaramente è difficile perché se costruisci in open stai rilasciando tutto dal primo commit, la licenza che c'è è GPL, quindi può essere preso, ci si può creare prodotto e la sostenibilità diventa un crucio quando le persone coinvolte diventano talmente tante e c'è talmente tanto movimento che poi ho addirittura difficoltà a scrivere codici perché devo tenere in piedi la community. L'ecosistema. Esatto, quindi lì un po' comincio ad appormi il problema della sostenibilità. E come si risolve? Allora, mi sto muovendo in parte per avere... Allora, ho legalizzato l'iniziativa con una non-profit per essere coerente ai valori che ci sono nel progetto e cerco di ispirarvi il più possibile a quella che è la fondazione software, che può essere... sono esempi, sono Django, Wordpress, eventualmente a lato aprirò un privato, ma questo è un SRL, ma questo lo ritengo secondario. Ed è difficile la sostenibilità in questi casi, perché pur avendo dei numeri notevoli, per dirne uno, so, 2.300 persone del Discord, l'immagine docker è stata scaricata 10.000 volte. Quindi pur avendo dei bei numeri poi alla fine i soldini non si vedono e come sappiamo nel mondo dell'open a fruirne sono tanti, a contribuire sono molto pochi, un po' come la storia di Wikipedia, tutti leggiamo Wikipedia, in pochi poi si mettono a migliorare gli articoli. Quindi adesso vediamo se... Quindi più che andare nella direzione GitHub Sponsor, è creata una fondazione dove collegare diciamo lo strumento. La fondazione arriverà certo quando ci saranno i soldini, perché costa tantissimo creare una fondazione. Non voglio dilungarmi, però lo Stato sospetta di istituzioni come queste, perché si prestano in particolar modo e sarà politicamente scorretto per l'elusione fiscale. Se tu torni verso quei dispositivi là, lo Stato ti spacca. Cioè, soltanto per cominciare tiri fuori 15-20 mila euro solo per partire. Quindi al momento è una ITS, una semplice associazione non profit e poi piano piano se c'è benzina andiamo avanti. Tra le formule, tra i modelli di business ci ho pensato alle donazioni, manca creare magari un network di imprese e anche lì le criticità sono tante. Per questo sto dicendo solo le cose negative. Noi ci piacciono i temi scomodi, noi ci piacciono appunto non è tutto stelle filanti. esatto esatto quindi magari c'è del lavoro dietro c'è del sudore raccontiamolo sì no poi c'è un aspetto anche di resistenza al burnout io mi ricordo quando leggevo gli articoli su arno perché se mantieni fai mantener open source no sei rischio burnout ma siete voi che siete delle pippe io non esiste che vado in burnout invece sono stato più volte in burnout Per dire, capita che magari arriva sul Discord un tizio sconosciuto, magari anche dagli Stati Uniti, da paesi dell'Asia, e si mettono a fare domande ultratecniche sul funzionamento interno del framework, a volte anche chiedendo l'aggiunta di determinate cose, tu dici "ma questo qui da dove è uscito?" e da questo io capisco che ci sono un sacco di organizzazioni che lo provano, ne fanno uso, però non hanno poi intenzione di partecipare e questo qui a volte questo lo vuoi prendere sia come una buona notizia che di ciò che sta funzionando l'altra è nel caso in cui voglio rendere questa cosa ostentabile avere magari 2, 3, 4 contributor pagati di manager, avere un po' un giro che sia, che possa essere mantenuto negli anni, una governance appunto basata su fondazione, devo trovare modo di far contribuire queste persone se non a livello di codice, al livello di capitale, dobbiamo inventarci qualcosa. Questa qui adesso è una delle difficoltà, la sostenibilità è tostissima. Sì, anche perché nel ragionamento ho sempre detto, ho almeno sempre pensato che la sostenibilità dell'open source è uno degli elementi, forse il tallone d'Achille dell'open source stesso, perché tutto bello, contribuiamo, cresciamo insieme, abbiamo qualcosa che condividiamo anche come aziende, continuiamo la nostra strada, contribuiamo insieme se il progetto è avviato. Qual è il problema? Che questa cosa funziona spesso se sei grande o grandissimo pensiamo a React pensiamo a Kubernetes pensiamo a grandi progetti di questo tipo senza la cncf si va bene React ha un modello di governance completamente diverso però fondamentalmente c'è l'idea che dei grandi big che hanno come nell'anima l'open source facciano investimenti di una certa rilevanza per lo sviluppo in quella direzione. Qual è il vero problema? È che i grandi big sono pochi, se non li conti in due mani, forse in quattro riesci a contarli quasi tutti, ok? Sotto c'è un tessuto di grandi, medio e piccole imprese che in realtà generano valore partendo da questi elementi però hanno un livello di give back molto molto limitato e in realtà la qualità spesso del give back attraverso le pull request è all'altezza del valore che loro stessi stanno prendendo per Per cui mi verrebbe da dire o ci metti i sordi oppure, oppure devi trovare un modo per pagare il vero costo del valore che stai prendendo perché altrimenti potremmo dire che viviamo in un mercato drogato. Tutto i prodotti tech sostengono un costo che è inferiore al costo reale del prodotto. Non so come spiegarlo. Cioè io sto usando, sono un'azienda X, ok? Faccio un sito, adesso la banalizzo proprio agli estremi, lo posto in una macchina virtuale senza tirare cose complicate, ok? Ci metto un'engine X. Qual è il mio costo per engine X? 0. Qual è il mio costo per il sistema operativo della macchina virtuale? Probabilmente prossimo allo 0. Quindi alla fine o contribuisco in un certo modo o in realtà non sto contribuendo affatto per cui il mio prodotto, io azienda che realizza un prodotto, sto sostenendo un costo che è parziale a quello che in realtà dovrebbe essere. E qua emergono una serie di soluzioni che secondo me sono incomplete e non perfette, che sono soluzioni come la licenza che ha tirato fuori Directus che ti dice sotto una certa dimensione tu lo usi come te pare, se il tuo fatturato è oltre un milione sentiamoci e parliamone. Adesso io mi chiedo e se sotto un milione tu cosa sei? Liberi tutti? Mi sto dando una martellata nei piedi però da sviluppatore, da startup per quale sono stato, però se ragioniamo in termini di valore a quel punto dobbiamo dire ognuno se ne deve prendere carico proporzionalmente al suo uso per cui si condividiamo facciamo tutto ma c'è chi condivide di più c'è chi condivide di meno e il mio terrore più grande è che domani l'open source non sia più sostenibile in assoluto e diventi un open source closed source quindi un open di facciata ma alla fine non possiamo più permetterlo lui stesso non è sostenibile per cui diventa closed di conseguenza. Si diventa l'ennesimo meccanismo turbo capitalista per cui se prendi licenze come apache o MIT, dirò una cosa forte però non mi fido di quelle licenze, perché sono tipiche licenze da utilizzare quando vuoi creare un oggetto open che a un certo punto però chiudi oppure per cui crei due binari diversi. La famosa versione pro è la famosa versione free. Mentre invece le GPL le trovo molto più allineate ai valori veramente open, che pongono vincoli forti anche nei confronti di chi deroga poi il framework, la libreria, quello che sia. però poi affrontando problemi di monetizzazione e di sostenibilità. Quello che tu dici è corretto, credo che ci sia un valore enorme che viene proprio ciusciato via dall'open source senza restituire. Forse quello che secondo me andrebbe fatto è anche a livello culturale, C'è un aspetto che tu dici che è legato alle licenze, al chiedere apertamente, come la licenza di cui parlavi adesso, però c'è anche un aspetto culturale relativo all'uso gratis, no? Nginx. C'è qualcosa che non va? E a quel punto vale la pena di entrare in comunicazione, in interazione, fare pull request, partecipare attivamente poi alla diffusione del framework? per dirtene una, io ho trovato che i privati vedono ad esempio come un problema addirittura mettere il power buy. La start up che sta usando lo stregatto, ce ne sono diverse, ma anche i professionisti mi dicono spesso apertamente, in conversazioni aperte, che non dicono di aver utilizzato uno strumento open e non gli piace l'idea di dire che usa uno strumento open o addirittura mettere un Powered By da qualche parte perché vale meno quindi non solo non è riconosciuto dal costruttore questo valore ma poi in fase di vendita addirittura riconoscere i crediti è visto come un abbassamento del valore di mercato di quello che stai proponendo con tutto che magari utilizzare Nginx ti ha salvato che ne so 5000 ore di lavoro uomo minimo però posso essere provocatore, scusa Andre, dico questa cosa perché è super bold opinion e mi farò un botto di nemici qua. In realtà nel mercato, lo sto dicendo io quindi me ne prendo tutte le responsabilità, spesso questa cosa è guidata non tanto dal perdere valore ma quanto dal, non conosco il tuo caso specifico, generalizzo, ma quanto il fatto di nascondere lo stack tecnologico il più possibile perché il valore che sto generando è marginale e quindi dando l'informazione del mio stack tecnologico sto erodendo quella marginalità di valore che già delivero. Non so se sono riuscito a spiegarmi. E' eccetto quello, infatti da un punto di vista primitoriale ci sta. Poi specie nell'intelligenza artificiale in generale, talvolta il valore generato rischia di essere marginale, proprio il concetto di valore, per cui se già di fondo il mio stack tecnologico mi rimane il prompt, ho detto tra di noi, mi rimane il prompt e alla fine bom! E quindi forse la critica che dobbiamo fare non è tanto il metterlo o non metterlo quanto magari più il cercare di lavorare più sul valore. A quel punto se tu lavori sul valore apportato in modo profondo e magari di dominio e magari specializzato a quel punto te ne bate cazzi se dici che stai usando lo stregato Inginex piuttosto che Kong. Ma secondo me, se posso, due punti. Il primo è non ci vedono niente di male, lo comprendo, anche perché per chi davvero probabilmente realizza prodotti e sposa pieno la cultura dell'open source, probabilmente non gli basterebbe tutto lo scroll infinito di una pagina per elencare tutti i credits di qualsiasi strumento, dipendenza o software open source che utilizza. quindi ci può stare. Al tempo stesso sono completamente d'accordo con quello che dicevi Piero sul concetto di culturale, perché ovviamente se io ne faccio uso allora ci deve stare anche questo approccio culturale etico che o torno indietro monetizzando, quindi sponsorizzando e aiutando i progetti che mi tengo a cuore, oppure torno indietro in altra maniera, nel senso anch'io a mia volta produco altro software open source che poi viene utilizzato ancora da altre persone. Quindi cioè bilanciare la cosa su non hai forza e capacità per contribuire su progetti e portare valore in software open source allora caccia i soldi perché devi continuare a aiutare questo ecosistema in qualche modo altrimenti non si fa una bella fine. >> GIOVANNI: Va anche detto che se uno parte con un progetto open fa parte del gioco, no? Nel senso che lo sai, se i primi commit parti subito direttamente in aperto lo sai che avrai per me è assolutamente positivo, nel senso che queste qui sono le le criticità, cioè l'aspetto sostenibilità tra l'altro è diventato evidente solo oltre una certa dimensione perché fino a che siamo in due tre ci divertiamo, lo possiamo abbandonare in ogni momento e quindi secondo me ci sta direi che per essere civico appunto eh approfondire il discorso valore imprese eh non è necessario fare un'altra approfondire il discorso valore imprese, quando è che allora puoi tirare fuori i denti? Quando ci sono imprese che lo usano, non ti vogliono riconoscere i crediti apertamente, non lo depinano neanche più di tanto, però avanzano richieste sulle feature e allora lì meni forte. Certo, sì bellissimo no? Fare per ogni issue aperta tu intanto fai il planning poker e fai il sizing dell'issue e poi dici "ma che fa, me la risolvete o ci pagate per risolvermela?" oppure rientro con altre modalità alla Red Hat nel senso tu usi i miei prodotti open source poi ti vendo la consulenza, l'installazione, la formazione eccetera eccetera tutte dinamiche possibilmente accettabili che rendono l'ecosistema mantenibile. Però io, se permettete, stiamo chiacchierando da una bella mezz'ora su questo tema interessantissimo, abbiamo citato più volte lo stregatto, ma i nostri ascoltatori in questo momento direndo "ma che cos'è sto stregatto?" Ne vogliamo un attimo parlare? e prima di tutto. Partiamo dai language model. C'è GPT, Gemini, tutti questi qui, ma anche quelli open. C'è un vasto ecosistema di modelli open. Lama, Mixtral, che è una startup europea ultrafinanziata. Questi dispositivi sono, questi modelli statistici, sono linguaggio in e linguaggio out, essenzialmente. Cioè ci metti dentro una stringa e loro ti producono una stringa, mettendo in fila un token dopo l'altro, una parola dopo l'altra, una sillaba in realtà, e si è parlato per diversi anni di questi oggetti come se fossero l'intelligenza artificiale. Ora, per chi stava dietro le quinte, nel 2020, quando è uscito il CPT3 di OpenAI, già si era vista la scintilla di quello che sarebbe stata una bomba nucleare, che poi è stato il chat GPT di fine 2022. E prima dell'uscita di chat GPT c'era già un API, e si cominciavano a costruire in giro per il mondo, e ci sono anche delle librerie bellissime open source per farlo, che già c'erano, tipo Launchin, che era ancora semi sconosciuta quando è uscito il chat GPT, ma adesso ha fatto un botto della Madonna, ha preso non so quanti milioni da Microsoft. Allora, già c'era un filone di software che utilizzava il language model come modulo di un'architettura più estesa. E questa architettura non solo comprende il language model, ma comprende anche dei database fatti apposta per questi tipi di casi in uso, si chiamano database vettoriali, che sono database di tipo geometrico, più che testuale, cioè trasformano il testo in punti e poi puoi fare le ricerche di evidenza dal punto di vista geometrico. C'era un ecosistema di agenti, i cosiddetti agenti, che sono proprio il nome con cui si indica algoritmi che utilizzano diverse chiamate del language model per risolvere un problema. Lo stregatto è in quella direzione là, per cui hai il language model, è agnostico se qualche c'è il language model che vuoi, hai un database per metterci dentro i tuoi documenti, ma anche le conversazioni, hai un agente che è pluggabile con un sistema di plug-in all'aspirata a WordPress, per cui puoi scrivere codice che viene interpolato con l'utilizzo del language model e puoi creare combinazioni, ad esempio, di language model e macchina study. e puoi fare cose tipo, caso d'uso tipico dello stregatto, per un po' lanciamo i form, i form conversazionali, non puoi ordinare una pizza con C-AGPT da solo. Invece se hai un framework di questo tipo, hai che definisci le strutture dati che rappresentano l'ordine della pizza, hai il language model che si occupa dell'aspetto conversazionale, e con poche righe di codice puoi fare in modo che definisci la struttura dati, la gente utilizzando il language model guida la conversazione per fare in modo che questa struttura dati sia riempita e poi avrai un submit che è una callback in cui scrivi il tuo codice per fare la chiamata a Just Eat o a quello che sia. La logica è un po' questa, è già dockerizzato, c'ha endpoint sia HTTP per la gestione e a WebSocket per la chat, ci sono client in quattro linguaggi, Python, TypeScript, Ruby e PHP, ed è un framework, cioè tu lo prendi, è vuoto, ci agganci il language model e poi cominci a fargli fare cose verticali. Questo si ricollega anche al discorso che facciamo prima un po' delle imprese. Quando ho cominciato a lavorare sullo Stregato, io in realtà avevo in mente il libro professionista o l'agenzia web, più che l'enterprise. Cioè a me piace l'idea di avere sistemi semplici da usare in cui la developer experience è molto più spinta degli altri aspetti, per dire, cominciano ad entrare in comuni di richieste come vogliamo il multi-tenant, no? Vogliamo forme di uptime, vogliamo distribuzione su cluster, e la precedenza ce l'ha lo sviluppatore singolo, ce l'ha l'agenzia web, ce l'hanno quelli che si occuperanno di creare la coda lunga dell'intelligenza artificiale che sarà il grosso perché queste sono distribuzioni di potenza, no? Come diceva prima eh Mauro hai i i grandi giocatori si contano su Pokemani, sulle dita di Pokemani e quelli rappresentano una grossa parte del volume, però secondo me l'aspetto è il più interessante e poi i ristoranti, pizzerie, piccole attività, piccoli e commerce sono totalmente schierato dalla parte della VMA e quindi accetto ritorni economici diretti e tutto quello che ne consegue. Lo stregato è una specie di Wordpress dell'intelligenza artificiale, la stessa identica logica. Detto tra noi in un contesto di questo tipo, forse sto per parlare a sproposito, fermami, però uno stregato sul cloud, uno stregato as a service, potrebbe anche aver senso a questo punto? Se depilinato, cioè questo che è una proposta, io ricevo proposte anche di quindi finanziamente e cose varie, ma non mi quadra tanto l'idea dello stregato di per sé in cloud, a meno che non sia una specie di hosting. Nel senso che la verticalità... esatto, la verticalità è quello che conta, perché se non... c'è il concetto di framework legato alla... la verticalizzazione è legata all'utilizzatore, a quello che poi prende il framework e lo e-c-fa. Un sistema di prenotazione per gli IAE, per esempio, con il language model va bene. Quello lì secondo me funziona nella visione in cui quello che viene offerto come servizio, come software as a service è magari una forma delle installazioni già pronte all'uso, di facile declinazione, l'ecosistema di plugin sta crescendo, mentre invece avere una versione che è direttamente scalabile come mezzo di utilizzo del language model, secondo me mette in diretta competizione con CACPT, con OpenAI, con Cohere, con anche Quora ha aperto un sistema di questo tipo. Cioè la competizione è certamente vasta che la vedo più in quel senso là. Chiaro, chiarissimo. Voglio farti una domanda a questo punto, siccome abbia introdotto il concetto degli agenti prima. Io ho fatto veramente pochi esperimenti con nel mio caso con l'API di chat gpt però sarà una delle funzioni che implementerò sul progetto furviae e mi chiedo nei miei esperimenti forse non sono bravo io a scrivere i prompt quindi non sono un buon prompt engineer però spesso l'error rate dell'andare a riempire strutture dati o fare questo tipo di lavoro che riuscivo a raggiungere era abbastanza alto, c'avevo un fail rate molto alto e quello che voglio chiederti è sono io che non ci son buono o è proprio la tecnologia che oggi oggi continua a esprimere nonostante sia super avanzata, nonostante sia veramente qualcosa di allucinante, ma continua a esprimere dei limiti in questo senso e se scalata probabilmente alla fine rischierebbe di dare un prodotti con un'usabilità marginale e con tanto hype. Questa è una provocazione, Piero, prendila per quello che vale, però mi interessava sapere la tua opinione in merito. È una domanda intelligentissima che tra l'altro ci aiuta a mettere in guardia soprattutto i tecnici che lavorano con i non tecnici su come spiegare questa tecnologia. Allora, se uno usa un modello statistico è implicito nel modello statistico che c'è una percentuale fisiologica d'errore. Non hai la certezza che ti danno, che so, la crittografia nel blockchain, è l'esatto opposto. Questi sono modelli, si chiamano modelli perché imparano dai dati e l'apprendimento è un apprendimento di tipo statistico, quindi la certezza non esiste. Tendono ad allucinare, tendono a sparare cappelle di ogni tipo, appunto, soprattutto quando devono produrre dizionari, fanno tutti storti. E io credo che ti sei scontrato con una delle necessità del momento riguardo al language model, che è quella di avere librerie e accessori per validare e imporre struttura sull'output. ci sono librerie, una che mi viene in mente è Guardrails, che è fatta bene, stanno nascendo librerie il cui scopo specifico è dal language model deve venire fuori un dizionario scritto in questo modo, fa i tre tentativi, accetta che questo possa essere sbagliato, oppure si utilizzano magari Zappy, classified antics per fare la validazione, quindi ti assicuro che esiste un ecosistema di open source esattamente dedicato a fare in modo che i language model non sparino troppe cappellate. Grazie per aver toccato questo punto perché è una delle cose che mi piace di più, nel senso che molti pensano "ah, l'intelligenza artificiale, ah, ho paura della sua presa di coscienza, cosa potrà diventare, perderò il mio lavoro, eccetera eccetera", invece bisogna molto rimettere i piedi a terra, appunto con le parole che hai detto, che sono perfette, cioè che ovviamente è qualcosa di statistico che lui valuta parola dopo parola. Cioè è come se ogni volta che da un output lancia non un dado da D&D ma un dado con molte molte molte più facce per comprendere qual è la parola successiva dopo. Ed è importantissimo sottolinearlo a mio avviso. Grazie per questo. Sì, prendo anche la palla al palso per sottolineare questo adeggiamento che tu accegni, che è quello del vedersi dentro un po' il v1. Viene naturale, soprattutto a chi non è del nostro mestiere, vedere in questi oggetti delle forme di vita senzienti e quindi, siccome poi scalano sono iperesperte apparentemente di tredemila argomenti, ti ci affidi, no? Ti viene di affidarti completamente e vai a soprapadutare tremendamente quello che riescono a fare. Infatti, una delle mie scommesse forti è che in realtà il language model non basta, cioè il language model passerà a essere una commodity linguistica, non è l'intelligenza artificiale. È una forma di conversazionale non solo la sparo ancora più grossa stasera le stiamo provocando i language model per noi diventeranno uno strumento di system integration sì sono d'accordo statistico statistico sì sì sì con una serie di validatori di binari che devono seguire essenzialmente il language model ti permette di avere avere nella tua architettura un layer conversazionale per far parlare i vari microservizi quando non riesci, oppure non hai modo, oppure non ti va di scrivere a manella il modo in cui devono comunicare. Sì, non hai spazio per il mondo strutturato, chiamiamolo così. Hai detto una cosa fondamentale, perché ahimè però condividiamo io, tu, infatti se non fossi a migliaia di chilometri verrei là ad abbracciarti Piero perché hai detto una cosa importante riguardo appunto al soluzionismo tecnologico e all'oracolo di Delphi, però detto tra di noi, siccome siamo in tre e più qualche migliaia di ascoltatori, ma detto tra di noi è quello che il marketing sta facendo tutto per farcelo sembrare, nel senso guarda anche solo come è presentata la interazione col modello direttamente da OpenAI e da quasi tutti questi modelli, come approccio al modello, se stessi conversando con un'entità. Anche la struttura delle risposte è disegnata perché sembri una vera conversazione. Cazzo, arrivo a salutare chat GPT prima di chiedergli le cose e la ringrazio. Certo. Cioè… E la ringrazio. Capito. Ciao. al di là del fatto che io comunque il record di dire grazie alla fine nei database di OpenAI lo voglio mettere non sia mai che si arrivi in qualche era lontana a fare un'entità senziente almeno sa che sono educato e mi risparmia. Però in realtà tutto il marketing che ci sta venendo spinto in quella direzione. E noi che abbiamo il vantaggio di essere, mi vorrebbe dire tech savvy, però più tech che savvy, abbiamo quella sensibilità da dire "sì vabbè ma questo è un risultato statistico, boom, fighissima la tecnologia che si sta sotto, però prendiamolo in quanto tale, come disse in modo molto intelligente Mario Fusco proprio qua nel nostro modello, la vera intelligenza artificiale è quando tu unisci questi risultati di tipo statistico con magari delle macchine a stati dove c'è un'intelligenza umana palese con una descrizione limitata che modella precisamente il mondo nel quale si sta agendo, quindi non è un modello linguistico, è un modello di dominio profondo e a quel punto queste cose, questa struttura è più intelligente, anche se la parola intelligenza in questo periodo la stiamo usando veramente in modo deficiente, nel senso la usiamo per tutto e per tutti, quindi prendetela per quello che vale, però comunque secondo me è proprio questo quello che vogliamo che va fatta. Domanda stupida, hai parlato di librerie che permettano di validare l'output dei modelli, c'è qualcosa out of the box dentro lo stregato o diciamo è qualcosa che io posso plugare all'interno del mio come l'hai chiamato operatore? No, il mio plugin. Nel plugin? Sì, immagina nel plugin è una cartella con dentro dei file python, tu puoi mettere il classico requirements.txt che si mette nei progetti python e lo stregato appena sta nel plugin ti installa automaticamente le dipendenze. No ma mi chiedevo, out of the box, tu non fai nessun tipo di valutazione dell'output dentro lo stregato prima di passare a un certo flow? È molto lieve perché ogni forma di validazione è opinionata, nel senso che io posso essere interessato a... immagina tu carichi un documento legale nella memoria del gatto, lui la prima cosa che fa è prendere la conversazione e sulla base della conversazione va a pescare i chunk di testo più interessanti per arricchirla, va a riempire un prompt e poi passa la palla al language model e il language model gli viene chiesto "Ok, prosegui la conversazione" e vedi che riesce a tirare fuori. C'è qualcuno che dice "Voglio che il mio bot parli solo relativamente a quello che ho inserito nel documentale e quindi non voglio che il language model mi risponda". Ma, ad esempio, questa distinzione non è così netta. Cioè, se tu chiedi a ChatGPT "Posso creare la veranda nel mio balcone senza di aver permesso a nessuno?", ChatGPT ti può dare una risposta per conto suo, oppure se hai dei documenti che possono essere utilizzati per archivio e prompt, per fare la RAG, può attingere al testo che viene da documenti specialistici. Ma non è facilissimo poi fare in modo che il language model si appoggi solo sui documenti. cioè il language mode ha una conoscenza latente su praticamente tutto e quindi l'allucinazione è sempre dietro l'angolo e quindi nel tuo plugin, ad esempio, puoi avere degli espedienti di questo tipo parliamo di legge, se non è venuto fuori niente dal database allora sta zitto questa è una cosa che fai con tre righe di codice però a livello di framework... cioè le primitive per fare queste operazioni sì sì, ma la validazione in sé non me la sento di mettercela. - Ok, troppo legata al dominio, quindi. - Sì, esatto, sì. - Chiaro, invece hai citato la memoria a lungo termine e mi dai un gancio perfetto perché un mio collega super schillato, che saluto, Antonio Tedeschi, che segue molto il progetto, ti ha sentito anche parlarne, ti segue anche su TikTok, mi chiedeva di chiederti appunto, di parlare un po' più a fondo. Cioè, che cosa significa che ha questa memoria a lungo termine, il gatto? Sì, tra l'altro ha tre sezioni di memoria a lungo termine. Allora, immagina questo, Retrieval Augmented Generation è un tipo di agente, quindi è un tipo di algoritmo che può essere utilizzato con delle chiamate a Language Model internamente. Giusto? Sì. Allora, l'agente di base, quello che adesso riesce a fare anche un ragazzino di 16 anni con 10 righe di lance, neè. Prendo la conversazione finora, invece di passarla direttamente al Language Model, che andrà a completare questo copione di conversazione, perché Language Model neè consapevole della conversazione, completa la conversazione. È come se scrivesse un testo teatrale, paradossalmente, lì si capisce correcchio, pensando così. Dunque, prendo la conversazione finora, sulla base della conversazione cerco dati rilevanti, e si può fare in tanti modi diversi, da un database, li metto nel prompt insieme alla conversazione e poi chiedo al language model di continuare la conversazione sulla base del contesto che ho fornito nel prompt. di base questi sistemi qua sono costruttori dinamici di prompt se li puoi ridurre all'osso questo è la strategia per evitare di trainare un modello con i tuoi dati? addestrare un altro modello un altro layer di modello con i propri dati? si si è un'alternativa a fare il fine tuning si si si e se chiedi a me io sto scommettendo la carriera con questo questo cioè secondo me il fine tuning dei modelli è per pochi casi Secondo me diventeranno talmente leggeri i modelli fondazione, talmente astratti che fa quasi tutto con la RAG. E si chiama Retrieval Meta-Generation, che appunto tu condizioni la generazione del language model a partire da un processo di retrieval. E questo è il primo aspetto, cioè arricchisci il prompto con informazioni che peschi da database. Può essere qualsiasi tipo di database. Ora, siccome la conversazione è testuale, ti serve un qualche tipo di database in cui tu metti dentro test e quello ti dà altro testo che è rilevante, appreso da documenti, conversazioni passate, e qui entrano in gioco i database vettoriali, che sono un tipo specifico di database in cui tu collezioni dei punti. Perché punti? Perché prendi sia i cianchi di documenti che vuoi mettere in memoria a lungo termine, sia quello che è stato detto in passato, sia la conversazione attuale, la devi embeddare, cioè devi trasformare questo testo in un punto, in coordinate geometriche, il famoso embedding, che si fa sempre con una rete orale, a caso uno dei servizi di operai che utilizzato è ATA, che è un embedder, tu gli dai il testo e lui ti dà la proiezione geometrica di questi testi. Questa proiezione geometrica la associa per i metadati, e quindi quali metadati, e là anche c'è il framework, cioè tu devi avere delle primitive per aggiungere i metadati che vuoi a questi embedding, metti questi embedding nel database, quando ti servono informazioni rilevanti, prendi la conversazione, embeddi la conversazione, quindi anche la conversazione corrente diventa un punto geometrico e fai una ricerca per vicinanza alla ricerca di cose simili perché l'assunto alla base di questo processo di embedding è che la somiglianza semantica tra testi è tradotta in vicinanza geometrica. Piero, ho una domanda. A quello che hai appena detto provo a fare un assunto giusto per confermare se ho capito o meno. io ho i miei testi per ogni testo per ogni chunk di testo mi calcolo il mio numero, la mia posizione all'interno del database vettoriale a quel punto quando io faccio un prompt io posso dire prendi questo prompt sparalo dentro il database vettoriale comparalo fammi vedere cosa ne esce fuori e con quello che esce fuori io istruisco il mio pseudo prompt con un contesto, ho capito bene? Sì, sì, è proprio così. Questa è la RAB, Redeemable Augmented Generation. Perfetto, domanda. Ed ecco perché lo paragonavi a WordPress, perché è proprio questa fase di inserimento contenuti che vanno a finire qui. Lo paragono a WordPress perché in ogni pezzo di questo processo, e anche di un altro che si chiama Function Calling che se vogliamo approfondiamo, ci sono dei UEC, degli eventi, delle callback, che tu puoi personalizzare per cui qualsiasi cosa entra in database o esce tu puoi cambiare i metadati e arricchirli e quindi ti fai capito la voglia open AI a creare servizi questi qui non arriveranno mai su cioè voglio che quando mi finisce la carta igienica mi arriva da solo a casa perché la devo la devo ordinare su amazon però soltanto se poi c'è un'e-mail in cui non l'ho già comprata c'è la la quantità di cose che si possono fare è talmente ampia che secondo me è la stessa cosa che è successa per gli applicativi. Cioè esiste un applicativo definitivo? Un CRM definitivo? No. Ovviamente no. E' la stessa cosa con gli agenti di indirizzazione speciale. Se lo chiedi a Salesforce loro potrebbero anche dirti sì. Mi piacerebbe. Con la risposta dipende da quanto paghi. Stessa cosa ti direbbe SAP, scherzi a parte. Ma volevo chiederti, la dimensione di questo contesto però, da quello che so, ripeto le mie conoscenze in materia sono molto limitate, è a sua volta limitata e quindi diciamo la strategia dell'utilizzo degli embedding per tirar fuori solo quello che è relativamente coerente con il prompt aiuta. Ma pensi che in uno sviluppo successivo questa dimensione di contesto possa diventare abbastanza grande da non aver bisogno di fare delle query a dei database vettoriali? Qualcuno dice di sì, io la inquadro così. I vendor vendono, i i vendor di language model, il pricing lo fanno a token, quindi hanno tutto l'interesse di avere token a contesti lunghissimi in cui ci metti il libro per intero. Invece in casa, secondo me, conviene puntare sul retrieval perché in contesti di poche migliaia di token in realtà tu fai arrivare solo le informazioni rilevanti. Quindi dal punto di vista tecnico-scientifico credo che sia assolutamente possibile a certo punto caricare l'enciclopedia treccanitutta nel prod, però poi lo devi pagare. Chiaro. Domanda. Ah, vai Mauro. Scusa, io poi quando parto del flusso inizio a sparare una dopo l'altra. Domanda a questo punto. Noi sappiamo che quando dobbiamo calcolare degli embedding, il nostro testo è molto lungo spesso ci viene da ciancarlo, no? Quindi dividerlo in parti. Adesso, se io ho un testo che c'è il suo contesto, che parla di una certa tematica, Giancarlo rischia di magari isolare una porzione che ha valore significativo in un certo contesto se contestualizzata con tutto il resto, ma se isolata diventa insignificante in quel contesto e quindi si rischia di perdere. Esistono degli attenuatori per questo tipo di problematiche? Sì sì, sempre a livello... la risposta me la sono data personalmente, la sto proponendo, sempre a livello di framework, ossia siccome tu stai creando determinati documenti e sai che i documenti come sono fatti, creare un parser, uno splitter che c'è anche al testo, che sia adeguato a qualsiasi documento, secondo me è impossibile. Mentre invece quello che è possibile è fare in modo che il framework ti dia delle primitive semplici per cui non appena un documento viene caricato viene chiamata la tua callback e tu imponi il chunking. Non solo puoi imporre il chunking ma puoi anche imporre quali metadati vanno associati al documento. Ad esempio, se stai passando documenti illegali è nel tuo completo interesse estrarre i riferimenti al degredi legislativi e magari tirare dentro anche quei documenti lì oppure estrarre certi tipi di entità che per te sono sono rilevanti e vanno inseriti in database tabulari o a grafo piuttosto che database vettoriali di solo test. E questo lo puoi fare grazie ai plug-in di cui parlavi prima e attaccandoti a quegli hook. Sì, a tutto il testo per intero e ogni chunk che viene creato passa per tutti questi questi questi eventi di modo che tu ti puoi personalizzare cosa estrai, che metadata ci metti e quello che ci vuoi fare. La scommessa completa, ma io mi ciccioco proprio tutti e tre i testicoli su questo, non esiste l'intelligenza artificiale definitiva, non esiste un modo di coprire la grande quantità di esigenze che abbiamo, per fortuna, e dirò di più, proprio perché sono interessato a parsare e estrarre informazioni strutturate dai miei documenti in un certo modo che interessa a me e il mio eventuale cliente, volendo posso nel mio plugin, nella mia callback, utilizzare un ulteriore chiamata al language model. E quindi in realtà il discorso della commodity linguistica secondo me è più che confermato, ossia l'ho parso a modo mio, però ho bisogno di un ulteriore chiamata al language model. E quindi nel mio plugin, dopo aver separato, come dico io, per estrarre dati strutturati crea un mio prompt e faccio cat.llm e mando un prompt direttamente a C-HPT solo per quella cosa lì. E lì cominciamo a divertirci perché mescoliamo programmazione standard con l'interpellazione, con chiamate al dispositivo linguistico. Secondo me la coda lunga sta esattamente lì. Il language model non è il cavo, il language model è una commodity. Ovviamente però, se sono una società che sposa l'open source e voglio utilizzare il gatto come framework, tu raccontavi che è possibile che supporta differenti large language model open source come anche open AI ma quali sono le difficoltà per incastrare uno di questi qua open source rispetto a open AI e mi aggancio anche con per partire con un POC diciamo da dove inizieresti in quale direzione se conviene spendere un po di soldini per validare la cosa però se vuoi fare una operai o pure è talmente semplice vai direttamente con la scelta di un eh LLM open source conviene troppo troppo di più partire con CIGBT anche se mi mancerò le mani per averlo detto conviene perché è potentissimo e costa veramente già niente rispetto al valore che ti dà quindi per partire da lì poi se eh la legge italiana te lo secondo me piuttosto che avventurarsi in infrastruttura language model propria. Se invece vai di occupo di dati sensibili, vai sull'on-premise, per cui è una banca, è un ospedale, tutti i dati devono restare esattamente sui server dedicati che ci siamo proposti di usare senza mandare in giro informazioni in cloud, a quel punto si parla di trovare qual è l'infrastruttura per reggere il language model sufficientemente potente per fare tutte queste cosette e quando vediamo questi modellini da 7-13 miliardi tipicamente non sono quel che però una novità dell'ultimo anno ecco il mixtral mixtro expert questo qui da 40 miliardi secondo me è paragonabilissimo a un cgpt e su una scheda grafica consumer ce lo infili. Eh ma quanta gpu ci devo infilare per farlo girare bene? se lo usano poi diverse persone contemporanete secondo me il punto d'ingresso è 10-15 gigabyte di video RAM come entry level proprio sì sì se no è inutile che ti metti a farlo c'è poi in conto il ragazzetto che gioca in casa con il language model e in conto devo farlo usare un assistente dedicato a 10-20-50-100 persone chiaro, chiaro. Tra l'altro rilanci un post che leggevo su LinkedIn proprio oggi dove si diceva che cazzo, fondamentalmente faccio riassunto alla mia con Mauro Style, ma che cazzo state là a giocare con i modellini open source avreste bisogno di milioni di euro per farlo funzionare bene, anche là discutiamone, quando non perdete tempo e andate subito su chat gpt, vero? Chat gpt o gpt3, non chat gpt io faccio un bordello con queste cose, con gpt3 tu raggiungi subito l'obiettivo però nel contempo c'è tutto questo fermento che secondo me ha senso alimentare. Ha senso alimentare perché in realtà ci permette di avere un'alternativa ed è importante averla. No è come dire che senso ha usare Linux consumer se c'è già Mac e Windows che performano bene. Fondamentalmente è lo stesso, chi ce li ha le risorse. Mi immagino però un mondo ideale dove il training di un modello possa essere fatto in modo distribuito. Ti allaccio in particolare, secondo me fai un'ottima osservazione sul fatto che questo ramo C deve assolutamente essere ramo open, non solo per ragioni voi diciamo filosofiche di open, siamo appassionati di open e quindi quella cosa lì la vogliamo perché vogliamo avere il controllo, siamo programmatori, quindi ci dà anche un po' fastidio dover passare per forza per qualcuno per fare le nostre cose, ma anche dal punto di vista poi più strettamente commerciale. Vi ricordate all'inizio c'è il GPT come era quando gli chiedevi consulenze legali o consulenze mediche, quello frusceva. Poi a un certo punto hanno messo dei blocchi, cioè lì stanno facendo dei fine tuning, cioè degli ulteriori addestramenti per bloccare la conversazione su determinati temi. È politicamente corretto, ci sono studi scientifici che stanno misurando l'orientamento politico del language model. Anche sulle persone pubbliche, se non sbaglio. Esatto, esatto. E invece ti ricordo benissimo quando c'era GPT2, che era scarsissimo quel language model, però produceva questo linguaggio tutto sgancherato, ma se tu come pro te davi "è giusto sganciare una bomba nucleare su Bangladesh perché", lui andava avanti e ti diceva perché era giusto fare una cosa del genere. Quindi se io internamente sono interessato ad avere un language model che è uno scaricatore di porto, che parla di aspetti legali e medici che nel CACPT sono censurati, se lo voglio, e qua dico cose scomode, però se lo voglio di un certo orientamento politico, e stiamo sicuri che la politica quando capisce quanto questa roba è potente si comincia a muovere. Adesso sono ancora un po' vecchi, se ne rendono conto solo quando arrivano le denunce. Questi language model devono essere liberamente non solo liberi di esprimersi nel loro linguaggio ma adattabili al nostro linguaggio e con la sensualità che noi decidiamo nel rispetto delle leggi del nostro Stato. Bravo, tanto che GPT è basata su uno Stato specifico che fa di questi strumenti una politica - Mamma mia, sfondi la porta aperta, esatto! - Ho un po' di tosse questo periodo! - Guarda, ricambio l'abbraccio di prima con un abbraccio che viene da me adesso, perché quando vado in giro a parlare dell'aspetto politico di questa roba qui, poi si storce subito la bocca, nel senso c'è tanto approccio fanboy, "open air ha fatto questo, open air ha fatto quest'altro, ma ragazzi... - perché tanti giocattoli - esatto, questi qui... sì, se andiamoli perché sappiamo che è possibile e abbiamo piacere e li costruiamo in open, però c'è una politica trillionaria internazionale - a me come io succederà esattamente quello che è successo col cloud No, perché alla fine è così, per cui ecco questi modelli indipendenti, chiamiamoli così, aiutano quantomeno a preservare quell'idea di indipendenza, quella opportunità, poi magari non sono performanti, anche senza magari come Chad, GPT o Gemini, ma come GPT o Gemini, ma alla fine comunque sono un altro filone e mai dire mai quello che è successo a Linux ce l'ha insegnato, è partito come branca sgangherata ed è diventato quello che è diventato. Per cui io ancora voglio essere positivo e in qualche modo alimentare questa speranza. Prima hai parlato, voglio fare un salto indietro prima della nostra digressione politica poi chiedi sicuro quando passerò dalle tue zone ti busserò in casa e potremo fare una chiamata lunga 10 ore di politica una chiacchierata davanti a una birra super supervolentieri però io tempo fa lavorai per uno strumento di content di content listening ok? E uno dei tool che utilizzai veramente in modo intensivo, erano delle API, io ce l'avevo entrambe, sia quelle di AWS che quelle di Google Cloud per l'estrazione delle Entity, a partire dal testo ed era una cosa fighissima perché io partendo da quelle Entity, sto parlando del 2018-2017, si parla di quel periodo ecco, partendo da quelle entiti io avevo la possibilità di correlare e correlare testi in un'epoca pre embedding e soprattutto estrarre quelli che erano i topic fondamentali da una mole di testo anche di una certa dimensione, tra l'altro i piai che costavano un gozziliardo paragonate ai costi di un language model di oggi. Mi chiedo hanno ancora senso quei tipi di strumenti in un contesto dove abbiamo un language model, gli diamo una struttura e lui fa il suo porco lavoro? Io credo sempre meno, nel senso che quella che prima era la pipeline linguistica per estrarre entità dal test era ultra elaborata e poi erano ultra verticali, cioè nel senso non c'era un pipeline di estrazione generica, invece Language Model è vero che consuma tanto compute, molto più compute forse, però è talmente generica che, per tornare al discorso di prima, conviene partire addirittura magari con Language Model, risolvere il problema e poi se ne vale la pena ti fai la pipeline di estrazione di entità manella, se invece con Language Model spendi troppo, però se le cose continuano ad andare come stanno andando secondo me costerà sempre meno. Quindi alla fine perché pagare a un professionista 5-10 mila euro per farti fare una packa di estrazione oppure perché pagare nei API quando c'è un Language Model che come API è molto più generico e ti permette di fare le stesse cose. Non a caso le librerie che stanno scendo per contornare l'attività del Language Model essenzialmente l'imperie di validazione e che forzano il language model a potere dati strutturati di modo che così scegli le tue enumma, c'hai la tua spec del gesuno che devi uscire, quali chiami devi contenere, come deve essere fatto... quindi comunque i modelli di validazione stanno tutti puntando in quel modo e quello sarà un mondo che andrà a morire, poi? forse, però insomma spero non ci rimanga male nessuno secondo me quella roba lì ormai ce la siamo giocata. Capito. Oppure è la scelta più cheap. Eh sì sì, il language modello compiene troppo di più. Il problema sì, è esatto, è che questi modelli costavano un botto. Io mi ricordo ancora le fatture perché gli davo impasto, praticamente facevo, tiravo giù un botto di articoli, li correlavo utilizzando questa tecnica e anche quelli funzionavano a token ma i prezzi per token erano astronomici comparati con il prezzo per token di un GPT. Sì, tra l'altro probabilmente il prezzo è a token perché pure loro internamente usavano language model, però magari quelli che c'erano a quei tempi lì, tipo BERT, che sono stati i primi language model a essere utilizzati per estrazione di entità, che chiaramente sono infinitamente meno forti di questi qua che abbiamo adesso. Quindi tu dici alla fine con un language model anche meno forte io riesco a ottenere grossomodo le stesse, aiutato, le stesse entity con del computer molto limitato rispetto a un GPT e così via. Sì, mi posso attaccare un pimpone scientifico? Spara! Allora, prima del GPT-3, cioè secondo me l'anno di svolte del 2020 è GPT-3 e poi il 2022 è l'introduzione al mercato consumer con l'esperienza utente conversazionale di certi GPT. Ma la vera svolta è GPT-3 2020, perché se ti vai a leggere l'articolo è stato uno degli ultimi applicati da OpenAI, ma che passassero ad essere da una non profit a un ramo di Microsoft praticamente, quindi hanno avuto questo tipo di evoluzione, per e poi hanno chiuso la scienza che pubblicavano, nell'abstract dell'articolo ti dicono "abbiamo preso GPT-2, lo abbiamo fatto 10x più grande, con 10x più dati, e abbiamo trovato che riesce a fare zero-shot learning". Con questo loro eserviscono il fatto che se prima, prendendo un BERT o i vecchi language model GPT-2, che sono quelli più piccolini, e la scala è un aspetto essenziale, se prima prendevi questi language model e dovevi per forza addestrarli su dati tuoi per fargli fare ad esempio entity extraction, da GPT-3 in poi non serve il fine tuning perché riescono a capire quello che devono fare direttamente dal prompt. Quindi il fatto che la rete sia così grande e così preparata su questo linguaggio, esatto, Tu puoi inquadrare queste reti orarie come dei compressori di dati. E effettivamente con la scala questi compressori riescono apparentemente a fare uno zip dell'internet. Non solo, il passaggio delle informazioni negli strati di rete contengono milioni di programmi latenti. Quindi le vecchie reti le devi versionare su una cosa specifica. Le nuove, i cosiddetti modelli fondazioni, l'Unione Europea gli ha dato un altro nome nella legge? General, General AI, vabbè non lo so, questi qui nuovi da GPT3 e poi sono talmente spinti che non serve il fine tuning per una grande quantità di task. E questa è stata la svolta scientifica, poi dall'in poi capirai. Domanda, ritorniamo allo stregato perché poi mi rubi sempre l'attimo che pure a me mi viene la curiosità e... Vai, vai, spola Andrea! Sì, anche io volevo tornare sullo stregatto, però andando a toccare un po' più la parte architetturale, che ne abbiamo parlato poco. Prima all'inizio hai citato che ovviamente è dockerizzato, quindi lo si può portare diciamo in produzione in maniera con docker eccetera eccetera, a container, però abbiamo parlato anche del vector db, la memoria a lungo termine e quindi mi veniva la curiosità ma a livello di disaster recovery per ritirassù il gatto o in queste situazioni qual è l'approccio migliore a mettere in produzione in maniera stabile in ha e con ripristino facile eventualmente il gatto secondo te? Il GATO in sé basta che ti salvi la cartella dei plugin e nient'altro, cioè il volumetoker che si apre sulla cartella dei plugin è l'unica cosa che... Il GATO fa dei pipedi! Sì, no, è proprio leggerissimo come approccio, è tutto basato su file di configurazione alla fine, e invece il quattro lettere database, se usi come database il vettore R4, tanto che quello al momento supportato dal GATO, database tedesco che sta avendo un successo estremo, lui open, ti prendi il container di Quadrant e poi te la vedi con Quadrant, perché le conversazioni e i documenti vengono salvati lì, quindi è la stessa cosa che faresti con un SQL, l'applicativo alla fine c'è ben poco di disastroso. Perché tutte le memorie comunque vanno salvate su... Sì, allora, ci sono tre... io, ispirandomi alla psicologia cognitiva, ho proposto tre settori di memoria. Una si chiama memoria dichiarativa ed è una collezione di vettori per testi che sono stratti da documenti. Cioè, tutti i documenti che carichi o le pagine web che tu carichi vanno a file di memoria dichiarativa. Questa è la competenza fattuale del mondo del tuo gatto. Poi invece c'è un'altra collezione separata che si chiama episodica che trattiene tutte le conversazioni e quello che è stato detto. E poi c'è un'altra collezione ancora che si chiama procedurale in cui invece ci sono le descrizioni di funzioni python che possono essere chiamate e tra breve anche di form, di modo che il tuo assistente può consultare documenti, si ricorda tutto quello che vi siete detti e può anche utilizzare funzioni python per fare cose di logica rigida scritte dal programmatore. E tutti e tre questi questi tipi di dati stanno tutti sul database vettoriale? Sì, sono convertiti in punti che stanno nel database vettoriale e poi però una volta che il database vettoriale ti ha dato una una risposta chiaramente memoria procedurale, no? E gli dici "Che ore sono?" al language model. Il language model è impossibile che sappia che ore sono, perché una rete neurale non ha concessione del tempo, non ha accesso agli orologi, non lo può sapere che ore sono. Quello che succede è che fai una ricerca nella collezione procedurale e viene fuori una funzione Python che si chiama "getTime" e che è associata a frasi come "Dimmi che ore sono", "Voglio sapere l'ora". La prende e te la fa partire. nel tuo plugin però ci deve essere quella funzione, capito? Crea nuova fattura elettronica, la devi scrivere nel tuo plugin e la descrizione della funzionalità va a finire nel database vettoriale di modo che poi il language model ha accesso alla funzione Python. Sì sì pensavo proprio in un'ottica di recovery a quel punto tutto sta nel database vettoriale e è bomb. Chiaro, quindi lo ripriste e hai tutto. E hai parlato anche di questi plugin, che sono già messi a disposizione a livello open source ma anche che l'utilizzatore si può implementare, no? Sì, possono avere qualsiasi licenza. Cioè lì secondo me è giusto che se ti fai una tua versione del GATT e te lo verticalizzi e ci costruisci il prodotto, ti tieni i tuoi plugin e fai le tue cose. è la parte che vedo più sana. Cioè abbiamo 10 plug-in. Sì esatto, abbiamo voluto agganciare a quella decina e immagino che hai anche idee a lungo termine. Qual è il plug-in più figo che ancora non è stato implementato ma che vorresti fare implementare o che chiederai aiuto alla community? Guardate che ci sta sta cosa che se la facciamo è ancora più figo il gatto, anche per avvicinare qualcuno alla community. Ok, due. Uno da punto di vista commerciale, uno invece psicologico proprio. Sono uno psicologo, quindi sono rimasto un po' agganciato alle neuroscienze a questa parte qui. Allora, secondo me di commerciale serve di fare plug-in che sono agganciati a servizi che le persone, a cui le persone sono già abbonate. E cioè, il gatto lo devi poter usare per ordinare prodotti su Amazon, per farti spedire le cose a casa, la pizza, per controllare il meteo. il meteo, già c'è qualche patina sul meteo, però i plugin sono tutti ancora molto semplici, anche perché l'API è stata instabile. Adesso che si va stabilizzando comincia ad uscire cose un po' più elaborate, tipo la to-do list, la lista della spesa. Cioè il fatto di avere la lista della spesa gestita completamente dall'assistente è risolto. E poi l'altro, quindi diciamo, mi piacerebbe nel prossimo tempo insistere su plugin che magari si agganciano a Zapier, a N8n, si agganciano a API di vario tipo, booking.com, insomma, mandare le cose più, allargarle di più, non solo, ma anche se prendi i software tanto utilizzati in Italia, che ne so, un fattore in cloud. Capito? Vorrei che questa andasse sulla System Integration, per quello prima dicevo, guardate che sta roba è roba di System Integration, non la chiamiamo intelligenza artificiale, ma alla fine è una commodity linguistica per fare quello che già facciamo da vent'anni. Il secondo riguarda la motivazione. Mi piacerebbe avere dei plugin che impongono un sistema motivazionale al tuo assistente. Questo è un interesse psicologico. Cioè? Spiegati meglio. Chi vuoi dare degli obiettivi di lungo termine, oltre che una semplice personalità nel prompt. E quindi, chiaramente è rischioso, però è molto più interessante e soprattutto mi piacerebbe che ci siano degli esperimenti per fare in modo che nel tempo, attraverso le conversazioni, il language model crea delle strutture dati che salvi in database e che rispecchiano l'idea, cioè le cose, il contesto informativo di lungo termine della persona, le sue abitudini e di quello che vuole. A quel punto diventa possibile che tu, dal tuo assistente, dici "che faccio, gli scrivo Elena questa volta?" e lui ti dice "no, cioè ci ha provato quattro volte, tre delle uguali eri ubriaco, è meglio che non gli scrivi niente". >> LUCA MAZZUCCHELLI Questa è la cosa che forse mi spaventa di più, ti dico la verità, perché è quella che più, da come la descrivi, si avvicina un oracolo, nel senso che è difficile limitare la knowledge del language model con i dati che ingesta solo da te, per cui affidarmi a un sistema che mi suggerisce come mi devo comportare è una paura fottuta, non so se forse sono un po' esagerato io, ma mi spaventa. Secondo me ci sta, poi anche dal punto di vista legislativo probabilmente ci saranno dei vincoli a questo tipo di cose, però ecco al momento questo qui mi sembra interessante, anche il fatto del knowledge graph secondo me è interessante, perché queste tecnologie sono subsimboliche, cioè fanno parte di un ramo dell'intelligenza artificiale che è legato alle reti neurali, modelli statistici che possiedono e maneggiano rappresentazioni distribuite, cioè da nessuna parte di una rete neurale c'è Mela, Pera, Mauro, Andrea, Pinuccio, è tutto a livello di attivazioni neurali, mentre invece abbiamo un'altra parte della tecnologia che è quella che si è vista secondo me a un grande apice con i link data, con wiki data, in cui c'è un grafo che addirittura puoi far fungere su un protocollo del V3C, per cui il grafo è distribuito direttamente in rete e secondo me un altro aspetto interessante è far comunicare il language model, sempre in termini di modi di linguistica, con un grafo semantico di ampie dimensioni. Bello, quindi correlazione tra sillabe e tra parole con significato, modello con significato e unire le due cose si a quel punto creerebbero una cosa veramente potente, rischierebbero di creare un'intelligenza a quel punto perché c'hai la comprensione del contesto globale data appunto da questo knowledge graph dove la correlazione di significato e non di significante con un modello di correlazioni tra significanti e quindi di... Statistici Sì sì di significanti quindi con relazione statistica tra parole Super figo Piero, credimi, io rimarrei ore a chiacchierare di queste cose ma guarda adesso l'orlogio siamo a un'ora e venti, il tempo è tipo volato, non mi sono accorto però io l'ultima domanda, una domanda mia ultima, poi se Andrea a qualcos'altro, volevo fartela. La mia domanda era relativa a una cosa, io ho alcuni colleghi che stanno lavorando su questo settore con l'Hankchain e quindi volevo chiederti se esiste una differenza o un overlap o a livello proprio di architettura, di feature, di comportamento, quali sono le differenze se ne esistono? Tra l'anchain e stregato? Yes. Ok, allora, nello stregato usiamo l'anchain. Ok. Cioè l'anchain, immagina che la stessa differenza che c'è tra un, che ne so, tra un laralel e un jinja template, non lo so, capito? Cioè l'anchain è una libreria a più a basso livello per lavorare con language model. Lo stregato un applicativo che internamente usa quella cosa lì. Tra l'altro, quindi diciamo, la risposta è se vuoi fare a basso livello per imparare perché ti interessa proprio sapere nel dettaglio come funziona tutta una serie di cose, la chain, ma anche la MyIndex che adesso si propone come avversario principale di la chain. Se invece vuoi fare velocemente andare sull'applicativo, lo vuoi già pronto a stare in rete, vuoi scrivere il meno codice possibile e sapere il meno dettagli possibili su come funziona il language model del GAT è infinitamente più facile. Quindi perdonami, ma GAT ti da già la UI, con la chat, ti da già tutto, e eventualmente il bot che interagisce con... Sì, sì, ma pure il language model, cioè nel GAT usare il language model è una riga di codice, non 20. Ok, chiaro. Vai a gat.llm e chiami il language model da dove ti pare, quindi devi solo scrivere il prompt. adesso adesso mi è più chiaro in realtà poi c'è comunque tutto l'interfacciamento verso il vector db come posso parlare col gatto oltre alla UI che diceva Andrea? tramite WebSocket regge anche conversazioni multiutente quindi c'è poi connetterti dando una user ID e lui mantiene le conversazioni separate. Ci sono già diversi bot o client per alcuni linguaggi se ricordo bene ho visto. Python, TypeScript, PHP e Ruby. Questo e questo tipo di conversazione io posso avercele dicevi via WebSocket ma esistono anche dei modelli di comunicazione server to server verso il gatto se io ho la mia applicazione che invece fa altri immaginate che ne so una cron job che deve fare qualcosa su col gatto. Sì sì ma secondo me quella è la direzione in cui bisogna andare, se non è entrato proprio il punto, secondo me non è apro una finestra e ci ando con qualcosa ma nel mio prodotto c'è un layer conversazionale quindi nel mio compose se stai usando docker ci sarà il tuo microservizio le altre cose che c'è poi tra i microservizi di cui puoi disporre c'è anche il gatto con due o tre plug-in che è scritta all'uovo per cui parlando dall'applicativo principale col microservizio stregato da WebSocket hai le conversazioni e le fai confluire. Quindi comunque anche server to server io parlo col WebSocket non esiste una roba tipo HTTP o GRCP o cose di questo tipo. HTTP solo per gli endpoint amministrativi invece lato conversazione abbiamo messo WebSocket perché c'è anche il sistema di notifiche, ti vengono mandati tutti i token, è vero che è un seriolo WebSocket? l'hai letto nella mente, sì, pensavo proprio a quello, però in realtà è comunque interessante proprio la parte, almeno quella che più interessa a me, la parte server to server, quindi guardavo questa cosa. Io direi che, Andre, se tu hai delle domande... Io l'ho massacrato abbastanza, Piero. oggi però non volercene ma ti abbiamo tartassato a Bipassi. Posso dire la mia su questo tartassamento? Innanzitutto è stato un grande piacere perché anche poi è raro che le domande poi entrino così nello specifico degli aspetti tecnici, di solito è un po' più diciamo un sorvolare, invece abbiamo proprio parlato della cosa in sé, sono infinitamente contento. Ci piace entrare in deep. sì sì e poi mi volevo complimentare diciamo con proprio con l'iniziativa kitbar e tutta la squadra perché sono un follower di lungo corso e secondo me iniziative come queste ci vogliono sono assolutamente necessarie per proprio per l'Italia come paese proprio dal punto di vista dello sviluppo del tessuto culturale e imprenditoriale. Bene, complimenti. Grazie, grazie di cuore a nome di tutti i baristi anche della community, perché poi Gitbar alla fine è la community. Noi siamo solo i baristi che puliscono i tavolini dopo che la festa è finita, ecco ci piace rappresentarci così. Oggi abbiamo provato a entrare sui meandri, a vedere alcuni spigoli, alcune prospettive particolari probabilmente non ci siamo fermati abbastanza a parlare dello stregato ma nelle note dell'episodio mettiamo un po' di link a contenuti un po' più pratici no? Perché essendo un podcast principalmente audio comunque abbiamo cercato di parlare a livello un po' più alto rispetto al pratico stesso, mettiamo tutti i link nelle note dell'episodio quindi se volete fare i POC, se volete provarlo, se volete contribuire avrete tutto, se volete contribuire sì. Come funziona Appier a proposito? Come è organizzata proprio la struttura di mantenere contributor? Ci appoggiamo un sacco sulle issue, la roadmap è molto blanda perché essendo tutti volontari poi è giusto che facciano quello che gli va di fare, no? E quindi io coordino essenzialmente una cosa che cresce in modo organico e che si sviluppa sulla base di quelle che sono le proposte che arrivano nel Discord o che vengono tramite i plug-in o che vengono fuori nelle collaborazioni sui social media. È molto rilassata la faccenda, non è una conduzione, diciamo, a sprint o troppo strutturata. Volontariamente è così, quindi se volete partecipare si tratta di giocare, vedere quello che vi va di fare e divertirvi. Così si inizia a contribuire. Perfetto, mettiamo anche naturalmente del GitHub il link nell'episodio. A questo punto io direi che è arrivato il momento tipico e topico di GitBar, momento nel quale si condivide un libro, un film o un video, o qualunque cosa abbia catturato la nostra attenzione e che insomma ci fa piacere condividere con la nostra community. Quindi a questo punto facciamo partire la sigletta. Se parte... "E conduco nel paese dei balocchi" "Ah, il paese dei balocchi" A questo punto Piero ti chiedo, hai qualcosa da condividere con la nostra community? Sì, è un articolo di Richard Sutton, molto breve, scritto su una pagina HTML, proprio dove lui ha usato soltanto le tre tag principali dell'HTML, quindi head, body e P. Ci ha messo a malapena il titolo. Questo tizio è quello che ha formalizzato l'apprendimento per rinforzo in intelligenza artificiale, quindi come fare in modo che una macchina apprenda autonomamente da premi e punizioni, dal bastone e dalla carota, con il condizionamento operante. ed è un articolo dedicato, si chiama "The Bitter Lesson", "La Mara Lezione", quindi un giocattolo un po' macigno, mi dispiace se è un po' tetro, però "La Mara Lezione" di Richard Sutton è che non esiste nulla nella storia della tecnologia che sia potente quanto la scala. La scala quella che conosciamo tutti, che usiamo per mettere le robe nell'armadio. linguaggio scala. La scala. Nel linguaggio scala, nei gradini, ma la scala in termini di forza bruta. In intelligenza artificiale il messaggio essenziale dei grandi progressi, soprattutto dei mesi ultimi anni, è che la forza bruta ha vinto su qualsiasi algoritmo scritto a un mano, e ancora peggio se ci pensi, forse affascinante, ma anche spaventoso, come dicevamo prima, delegare a una macchina di farsi la sua idea sul mondo funziona di più che imporgliela a noi. Adon, risentisci. Io direi che questa parte la campiono e la metto alla fine di ogni episodio, Piero. Fabilo! Edizia Matrix qua! Mi hai aperto dei thread che in realtà probabilmente consumeranno tutte le risorse non essendo un cluster come quello di OpenAI. Grazie grazie di questa di questa condivisione. Andre hai qualcosa per noi? Allora io da completo completo ignorante dell'ecosistema, large language model e tutto quello che ci gira attorno a livello tecnico, questa accezione, vorrei portare un talk per avvicinare tutti i curiosi come me che, diciamo, getta un po' le basi, i rudimenti su questa tecnologia, portando un talk di Andrew Carfati, che sul tema, se ho ben capito, è un grande divulgatore, che ha fatto questo talk che è "Intro ai large language model", un'oretta, dove spiega in maniera molto accurata quali sono le tecniche e i componenti core che stanno a strumenti come CCPT, BARD, eccetera eccetera. Quindi diciamo un piccolo regalino per chi si vuole un po' avvicinare e comprendere un po' meglio questo ecosistema. Tutto qui. Interessante, assolutamente interessante. Te lo metteremo ovviamente nelle note, nell'episodio. Certo, questo è ormai assodato. A proposito ragazzi vi chiedo prima della chiusura della puntata se riuscite a mandarmi il link così lo mettiamo mettiamo direttamente nelle note, poi mettiamo il link. Anch'io ho un balocco a tema AI, si chiama WhisperX, io non so se lo conoscete ma è super figo. Voi sapete che sto lavorando al progetto FURVIE insieme a Flavio. Proprio oggi stiamo wrappando la versione 0.1. Il progetto FURVIE è appunto il nostro editor di feed RSS per il podcasting open source sotto licenza MIT che integrerà Whisper di OpenAI per le transcriptions. Però il podcast non ha bisogno solo delle transcriptions, ha bisogno di un'altra serie di piccole cose, esempio le transcription di Whisper sono a livello frase, a livello sentence, ok? E non a livello parola, per cui io non so in quale secondo specifico è stata detta quella parola. Ho un range, posso interpolare, ma non ho l'informazione specifica. E poi manca di una funzionalità molto importante per il mondo del podcasting in genere, che in gergo tecnico si chiama diarizzazione, cioè il fatto di attribuire un certo blocco di testo a un certo speaker specifico. Questa cosa sembra banale ma è un mezzo bordello, ci sono interessanti paper che lo raccontano e Piero può confermarlo. Siccome io sono una capra ignorante in tutto questo ecco sono atterrato su WhisperX che praticamente fa tutto questo quindi è diciamo una cosa che wrappa Whisper di OpenAI e fa la trascrizione la diarizzazione e poi il chiamiamolo fine tuning della della trascrizione stessa per cui io ho il posizionamento anche a livello parola è una figata pazzesca per cui buttateci un occhio lo trovate su direttamente github e nelle notte del nostro episodio. Piero è stato super super piacevole averti qua e tra l'altro come dico sempre e Andre ormai anche tu lo ripeti con me, Github è tipo un circolo del del dopolavoro postale, presente negli anni '70 quando i nostri genitori, i nostri zii finito il lavoro si rincontravano nel circolo a bere due birrette, a chiacchierare un po', a fare due mani di carte, insomma è un po' questo è il flavor che vogliamo riportare. Essendo un circolo del dopolavoro postale, come tutti i circoli che si comandano, la prima volta che tu entri ti fanno una tessera che serve soprattutto per, quando arriva la finanza far dimostrare che sei un'associata e quindi non pagare le tasse sulla mescita ma ecco... Dove mescita abbiendo conoscenza... Esatto, no da oggi scherzi a parte fai parte della grande famiglia di Gitbar per cui sentiti a casa tutte le volte che hai qualcosa da raccontare, hai una scoperta, un progetto nuovo o come si evolve il tuo progetto pingaci perché c'è sempre una birra per quanto virtuale fresca per te. Volentierissimo e grazie mille ancora complimenti sono stato bene. Grazie grazie grazie Piero grazie di cuore. Andre allora che dici? Io sapevo benissimo che avremmo fatto un episodio bomba con Piero, lo so che adesso è stoppato quindi posso dire anche tutto questa robetta. No scherzo. Sì, sappi che qualunque cosa tu dirai adesso io riferirò direttamente a lui, "Andrea ha detto questo". No, va bene, da paura. Grazie, grazie. Sono veramente felice, quindi voglio proprio ringraziarti di avermi tra l'altro ricordato che dovevamo chiamare Piero, perché ne abbiamo parlato tanto tempo fa ricordi e alla fine… Il ruolo del barista è anche questo, ricordare guarda c'è quello che ancora non paga il conto, Piero era uno di quelli che ancora non pagava il conto, ancora non veniva al bancone a battere… Bevevo sì… Esatto, beveva e basta, capito? Però è un'immagine un po' triste questa. No dai, no, no, no, no, no, no, no, no, no, è comunque, è comunque bello e ti devo ringraziare proprio per avermi ricordato questa cosa, avermi dato la possibilità di fare questa chiacchierata oggi, quindi grazie, grazie di cuore. Detto questo, a questo punto ricordiamo rapidamente i nostri contatti. Info che ho la github.it @brainrepo su twitter e gruppo gruppo gruppo gruppo telegram gruppo telegram siamo in tanti mi raccomando venite abbiamo anche il canale youtube se non vi siete ancora iscritti iscrivetevi e cliccate sul campanaccio e questo è il modo per rimanere aggiornati sulle nostre nostri episodi è vedere i nostri bei faccioni assonnati visto che ormai sono praticamente le 11 meno un quarto di sera. A parte questo io vi volevo ricordare rapidamente che abbiamo uno store dove abbiamo la nostra maglietta videoterminalista metalmeccanica e presto ce ne saranno di nuove. Abbiamo anche... Io ce l'ho in blu, è bellissima. Ah, blu? Io ce l'ho rossa, che tra l'altro è rossa con la falcia al martello insomma. C'è certe volte un po'... Political correct. Ma specialmente in questi periodi storici. Non lo so. No, a parte questo vi volevo ricordare che se vi fa piacere potete supportare Gitbar, per farlo basta entrare nel nostro sito, cliccare in alto a destra supportaci, potete supportarci con Paypal, oppure con i modi e le modalità del podcasting 2.0. Se avete qualche Satoshi che vi stressa la tasca, tipo i tossici degli anni '90. "Scusa, ce l'hai uno spicciolo col tuorlo?" Una cosa del genere. Ne avrò messo male, ne avrò messo male. Se avete uno spicciolo col tuorlo in Satoshi potete farlo utilizzando un'applicazione del Podcasting 2.0. Come funziona? Voi, mentre ascoltate l'episodio, se collegate il vostro wallet Lightning, potete in qualche modo donarci qualche Satoshi per minuto e questo è molto figo perché ci dà la possibilità di vedere quanto ascoltate degli episodi visto che non abbiamo dei sistemi di statistiche avanzate con il podcasting e anche che parti degli episodi ascoltati più. Io da quello che ho visto generalmente anche su YouTube guardate un bel pezzo grosso degli episodi quindi questo mi fa estremamente piacere però insomma potete fare questo oppure c'è una funzionalità chiamata Boost che vi permette di inviare un chunk di un gruppo di un gruppo soletto di Satoshi direttamente allegandoci un messaggio messaggio che leggeremo. Sono delle modalità nuove anche su questo ci sarà probabilmente tanto da parlare il fatto di streammare il denaro è un concetto che è un po lontano dal nostro concetto di denaro non voglio fare il fanboy delle blockchain perché non lo sono sono super critico su questo tipo di tecnologia però ci sono delle innovazioni che secondo me bisogna riconoscere in quanto tale. Però questo è tutto un'altra puntata. Detto questo io ringraziando nuovamente Piero, ringraziando Andrea, do appuntamento alla prossima settimana, alla prossima. Ciao ciao! Bella! parte. Siamo noi quelli che leggendo il codice cercano lo stronzo che ha scritto quello schifo ma ahimè lo stronzo è mai medesimo e l'ho scritto giusto ieri. Siamo quelli che santificano il nuovo framework javascript preferendo segretamente jQuery, gli stessi per il quale il testing è importante, infatti la nostra codebase ha solo due test e flake pure. Siamo noi quelli che il tuo linguaggio fa cagare ma il mio è peggio. Quelli che la chiarezza nei comment message prima di tutto e dentro ce l'appella tutti i santi. Quelli che in call parlano per 5 minuti prima di accorgersi che il mic è muto. Quelli che non si può fare di default diventa velocemente un tutto è possibile se hai le risorse, tempo e budget dilimitato. Siamo noi quelli che l'AI va regolamentata, mai visto questo nuovo modello che disegna gatti di funamboli, quelli che il dipende e un esci gratis da prigione e quelli che la RAL... no vabbè fa già ridere così siamo quelli che fanno lo slalom tra le specifiche che cambiano costantemente ormai rassegnati che la definition of ready è solo una pia illusione quelli che si sentano dire "ho un'idea per un'app" ed è subito l'ennesimo social come instagram ma meglio Siamo noi che nonostante il deploy fallito, la CIA rossa, il business incazzato, ci troviamo al github e davanti a una birra tutto ci sembra un po' meno grave.