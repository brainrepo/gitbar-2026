Bene e benvenuti su Geekbar, nuova settimana e nuovo episodio qua nel nostro bar degli sviluppatori. Le temperature sono ormai diventate estive, io ho acceso il ventilatore qua e non so se entra nel microfono. Se sentite questo ronzio sapete che non ci sono le zanzare nelle vostre cuffie ma è solo il mio ventilatore perché qua siamo a 32 gradi e mi raccomando, piccola premessa, qualche puntata fa vi ho detto vi ho suggerito di comprare un monitor ultra wide tipo quelli di Samsung 49 pollici. Ecco, non fatelo! Non fatelo! Sono delle stufe alogene. Io sto schiumando davanti al monitor. Certe volte preferirei andare solo con lo schermo del portatile. Detto questo, io vi ricordo rapidamente i vostri contatti. I nostri, vostri. Vabbè, i contatti sono anche vostri, visto che siamo una community. allora iniziamo dal contatto che condividiamo e infatti il nostro gruppo telegram il gruppo nel quale ci incontriamo tutti i giorni e chiacchieriamo del più e del meno in modo più o meno divertente il gruppo si chiama github podcast lo trovate nel vostro client di podcast quello di sì nel vostro client telegram perché un gruppo telegram se non vi siete iscritti perché io so che molti di voi non si sono ancora iscritti oggi non vuole partire ragazzi oggi non vuole partire. Fatelo siamo 1306 membri ad ora quindi andremo sempre a crescere. Vi ricordo rapidamente gli altri nostri contatti info@gitbar.it @brainrepo su twitter e www.gitbar.it. Ho aggiornato anche gli ultimi episodi con le trascrizioni quindi dovreste trovare praticamente tutto all'interno del nostro sito che da qualche settimana forse qualcuna in più di qualche permette la ricerca all'interno dell'episodio quindi se non una ricerca testuale all'interno dell'episodio quindi se non l'avete ancora fatto facetelo perché è carino andare a trovare le parolacce che diciamo o quante volte abbiamo citato react insomma può essere divertente però bando alle ciance è arrivato il momento di presentare il super ospite di stasera Benvenuti su GITBAR il podcast dedicato al mondo dei full stack developer, i mezzo artigiani, mezzo artisti che ogni giorno infilano le mani nel fango per creare nel modo più efficiente possibile quei prodotti digitali che quotidianamente usiamo. Questa era una puntata che volevo preparare da un po' ed è una puntata speciale. È una puntata fatta in collaborazione con gli amici di Code Motion e l'output di questa puntata non sarà solo l'episodio a sé stante ma ci saranno anche, sarà una delle carte da collezionare che potrete andare a raccogliere a scovare durante la prossima conferenza di Code motion quindi mi raccomando ascoltatela con gusto perché l'ospite è un super ospite e una volta ascoltata a code motion saprete che card andare a cercare oggi ho con me alberto massidda cognome che mi piace tantissimo alberto sei produczione ingegner a meta sì a tutti quanti grazie per mi sento un pokemon comunque anche una card è un pochettino bellissimo non ce l'avevo pensato non ce l'avevo pensato pensiamo a una cosa in realtà alberto leggo production engineer il ruolo di production engineer cosa consiste? C'hai presente il ruolo di, se io ti dico site liability engineer, SRE, ti dice qualcosa? Sì, sì. È esattamente la stessa cosa. Protection engineers sono gli SRE di Facebook. Cavolo, io mi perdo sempre su questi ruoli e non so se dipende da me che sono una capra ignorante o se iniziano a essere veramente tanti. Iniziano a essere veramente tanti, però è anche comprensibile perché comunque andiamo veramente avanti, cioè non sarebbe pensabile che ci fosse una persona a fare al massimo livello tanti ambiti insieme. Mi ricordo quando ho cominciato nel 2008 mi permettevo di fare svariate incursioni che andavano dalla hardware, il sistema operativo, il database, il back-end, il front-end. Cioè io facevo le interazioni jQuery sul prodotto che programmavo, mi sentivo super mega Haxor perché perché facevo le chiamate Ajax con JQayGet post, mi sentivo fichissimo. A fare gli spinner che poi sparivano, gli facevi hide se vedeva il risultato sulla pagina. Adesso non sarebbe assolutamente pensabile che io mi mettesse a fare full stack, a fare io le interazioni in JavaScript, ma proprio no. E quindi è normale che ci sia una fermentazione dell'organicram delle competenze di un team. Magari ti distingo perché comunque la persona che fa infra è ancora la persona che fa infra, la persona che fa front end oggi magari è quella che ti passa a fare sia mobile che web probabilmente. Anzi, marciamo proprio verso questa cosa. Spesse volte. Questo è il modo in cui si fanno le cose. Sì, pensavo però una cosa, visto che è bello iniziare subito l'episodio off topic, Fantastico però però ci sta, questa è l'anima di Gitbard. Pensiamo a una cosa sai, intanto la nostra industria si è evoluta, oggi proprio la struttura dell'industria è completamente diversa da quando abbiamo iniziato. Io ho iniziato 15 anni fa, immagino tu più o meno gli stessi, e alla fine è completamente diversa, si è andata a strutturare, i ruoli si sono definiti. Quello che però e quindi come analogia mi viene subito da pensare al mondo della medicina no? Cioè il gastroenterologo, cioè il cardiologo e un po' potremmo paragonare questi due mondi. Quello che però in realtà è strano nel mondo dell'IT è che spesse volte abbiamo nomi diversi per identificare lo stesso ruolo? Allora, ammetto che mi hanno cercato di spiegare la differenza una volta tra l'SRE di Google e Protection Engineer di Facebook, però infine la differenza è praticamente irrilevante. Nel senso, probabilmente loro cercavano di... se c'è stata mai una differenza, era quella che gli hanno cercato di dare all'inizio, ma nella pratica il ruolo viene interpretato stesso modo, cioè noi siamo quelli che fanno l'automazione di affidabilità. Noi facciamo i dashboard, facciamo tutta l'armistica, facciamo tutte le prestazioni, cioè prendere una roba che funziona e farla scalare a 100.000 quail al secondo. Come si fa? È possibile? Siamo quelli che si fanno queste domande, siamo quelli a cui importa di queste cose, siamo dei software engineer fondamentalmente, a cui importa solo di requisiti non funzionali. Questo è il modo più sensato in cui riesco a buttarla giù. Bellissimo però, sei riuscito a rappresentare un'immagine abbastanza chiara del ruolo. In realtà è molto difficile poi capirne profondamente. Io posso fare l'esempio in azienda dove sono io, è il delivery architect. Cosa sono? sono un ibrido tra un software architect, un delivery manager e un senior engineer con dei ruoli da lead e in un contesto di business funzionano bene, però in realtà inquadrare il ruolo diventa come... è praticamente quel grafico a... come si chiama? A parabola, no? È un po' il regista del progetto, no? Il delivery architect a questo punto che si prende la responsabilità di quello che infine finisce sul tavolo del cliente finale. Sì però per esempio spesse volte era il lead che faceva queste cose nelle aziende un po' più piccole con responsabilità formalizzate diverse. Insomma è un po' un bordello. Ma il lead è un po' nel suo, quello che noi chiamiamo tech lead oggi, c'è il tech lead di back end, di front end, di infra, chiamato DevOps, spesso è volentieri. Quindi magari il tech lead è una figura molto di verticale, per come la intendo io, il delivery architect per come le buttaggio. Tu mi sembra proprio la figura di accordo che si prende la briga di fare in modo che tutte queste figure collaborino assieme e la cosa vada in porto, tipo regista. Mi viene in mente un'idea, la butto qua alla nostra Unity. Mi senti? Sì sì. Dicevo mi viene in mente questa idea così forse pazza, non so se vi ricordate i vecchi videogame di calcio dove ogni calciatore aveva le sue statistiche, aveva quel grafico a lato che era una sorta di esagono. Picci calcio. Qualcosa del genere, esatto. Sarebbe bello fare una roba del genere con da una parte le tematiche, leadership piuttosto architecture piuttosto, per i vari ruoli dell'IT. Pensiamoci perché può essere un progettino carino, una sorta di top. Design ci mettiamo, si si ci mettiamo una serie di caratteristiche di quelli che sono super carenti. Sarebbe, sarebbe figo, sarebbe veramente figo farlo però voglio ritornare al topic di oggi e voglio arrivarci con una doppia capriola. Nel lavoro che fai tutti i giorni da production engineer, quanta artificial intelligence c'è? Allora, bella domandina. Tecnicamente zero, perché in teoria uno può tranquillamente e poter pensare in termini di affidabilità e prestazioni di un sistema farlo da solo, totalmente umano. Nella pratica però, uno può farsi assistere in un bel po' di cosine. Per esempio, mi immagino tutto quello che è la modellazione dell'andamento del sistema, tutto quello che è i vari data point, perché l'applicazione mentegia crea una serie di metri che quanta CPU consuma, quanti sono i fallimenti, quante sono le transazioni aperte, chiuse, eccetera, eccetera e le collezioni le mette in una serie dati a quel punto tu metti una sorta di predittore che la cosa si fa di solito, si mette un predictor che ti modella come va praticamente la serie dati e vede che se esce fuori dal seminato per una certa fascia oraria magari fa scattare qualche cosa Oppure un production engineer spesso si trova senza entrare nella AI a pensare quali sono i requisiti che un sistema che fa AI, cioè non viene assistito alla AI, ma si deve preoccupare del sistema AI che fa le cose come debba funzionare per essere veloce in maniera tale che una predizione, non ci metta un minuto di orologio a essere servita, ma tu in realtà vuoi chiederti come faccio a ottenere 100.000 query al secondo e quando lavori in metodo di alto questo tipo di scale sono assolutamente fattibili e non è poi che puoi averci 10 milioni di macchine che fanno questa roba. Le macchine che ti vengono date sono più o meno quelle, non c'è niente di limitato e quindi devi un pochettino chiedere ok che cosa voglio sacrificare, voglio sacrificare voglio sacrificare, deve essere una cosa che va in tempo reale, perché allora tu devi fare diverse scelte per fare in modo che questo benedetto sistema di AI che deve fare cose, sia una cosa performante, l'utente non si accorga che sta aspettando attraverso vari processi. Ti indico per esempio uno di cui mi occupo direttamente, io lavoro in una, immaginare che l'azienda divise in tante organizzazioni, in tanti pilastri praticamente, e la mia organizzazione è l'organizzazione dell'integrity. Io sono la polizia di Facebook, noi siamo quelli che trovano e rimuovono i contenuti nocivi che la gente posta sulla piattaforma. In particolar modo la mia squadra è quella che si occupa di fare similarity detection, cioè noi ci occupiamo di vedere se la roba che la gente posta già era stata classificata come... che viola le norme della comunità, e quindi ci adoperiamo per fare il motorbank eliminato, una sorta di enorme antivirus, se vuoi vederla così. C'è un file di definizioni che contiene da una parte tutto ciò che è male, che è già stato classificato come male, ed è audio, video, testi, ed dall'altra parte un motore di scansione che scansisce tutta la roba che arriva, per la cronaca parliamo di circa un miliardo duecentomila oggetti caricati ogni giorno sulla piattaforma. Non si vede la mia faccia ma quando hai detto il numero. All'inizio c'è de peggio in giro, nel senso una cosa è il fatto che gli astutenti carichino un miliardo e duecentomila di contenuti ogni giorno che vada scansito e che succede se una roba che hai caricato n tempo fa, oggi succede che è passata, tipo una madre che ha allatto al seno, arriva una enorme comunità che dice "no, non si possono ripostare foto di donne che hanno allatto al seno" e a questo punto tutte le foto di allattamenti al seno di adesso diventano prebite. Ma tutto quello che hai creato fino adesso, quanto pesano 15 anni di caricamenti? Quanti immagini sono? Quanti trilioni di immagini sono 15 anni di caricamenti a quella frequenza e di andarle soprattutto a scansire per essere sicuro che la roba che sta lì venga fatta fuori. Questo è un altro problema che il production engineer si mette le mani nei capelli e dice "oh mamma come faccio la cosiddetta retro action o backward scan" e quello è un altro bel problemino che il production engineer perché tu non puoi semplicemente fare "ecco l'algoritmo, metti una foto, io faccio un bel fingerprint, arriva il tizio e ti dice "vedi, prendi questa foto, la faccio diventare un vettore di numeri" a quel punto tu fai una ricerca vettoriale, in uno spazio vettoriale, vedi quali sono le foto più vicine a cui atterri, questa è la foto query, è atterrata vicino a quest'altra foto che io ho detto che era brutta, oppure è atterrata vicino a una foto che io ho detto che era bella, non c'è nessuna informazione, è benigna, e allora aziona, cioè fa la pezzi e fa anche il checkpoint dell'utente, banna l'utente che l'ha caricata, oppure lasciarlo passare. Quando tu pensi che devi fare la scansione di questi bei trilioni, sono milioni di milioni di foto che ci sono dietro, o anche di video, video poi, bellissimo, tutti i frame di un video devi farti. Non è che un video è un solo oggetto, un video in realtà sono migliaia di oggetti. C'è anche l'audio, ci sono i testi, tutte quelle cose ci sono, e questa roba deve avvenire in degli intervalli che devono essere tolleabili. Tu non puoi tolleare che una roba brutta rimanga sul sito solo perché stai ancora a fare la coda. Per accidenti di energia, tipicamente, nella I, arrivano questi modelli estremamente sofisticati che fanno cose, magari sono lentissimi, e dici "ok, come facciamo a farle funzionare?" Ti chiedi questo. Per accidenti di energia assiste la I, non l'inverso la maggior parte delle volte. chiaro pensavo a una cosa mentre parlavi e mi è rivenuto in mente quello che mario fusco disse un po di tempo fa in uno dei nostri episodi no cioè che talvolta la I quando mi ha detto abbiamo una serie di regole di di liste e ho pensato talvolta la I si può sposare anche con una serie di regole di dominio con un rule engine per dare creare un percorso più più completo in modo che l'uomo abbia il controllo e questi due tool insieme dovrebbero riuscire a limitare i lack le mancanze l'uno dell'altro no perché il rule engine ha delle mancanze l'intelligenza artificiale ha dal canto suo delle mancanze perché non capisce l'intrinseco significato delle cose. Poi ti faccio infatti una domanda su questa cosa. E quindi questi due si compensano come disse lui. Però quando noi parliamo di AI parliamo di un mondo infinito di topic diversi anche. Iniziamo da quello uno dei più noti ed è il machine learning. Quando si parla di machine learning e si parla di algoritmi di machine learning si parla di algoritmi di tipo supervisionato o meno. C'è anche la via di mezzo adesso, i self supervised, gli autosupervisionati. Vai voglio sapere tutto sappilo Alberto. Allora proprio volendo partire un pochettino alle basi un modello di machine learning è nient'altro che un modello matematico che ricalca una distribuzione su un insieme di dati, per esempio l'insieme di dati di quanto è alta l'argente. Tu prendi tutti i persone d'Italia, le registri su un bellissimo csv e le dai in pasta a un modello e tu puoi tranquillamente creare una rappresentazione statistica di quanto è alta l'argente, quanto è facile quando si è alto 2 metri, quanto è facile quando si è alto un metro e mezzo, più o meno l'argente si è alta almeno 70, almeno 80. A quel punto tu puoi sapere come sono distribuite le cose, puoi anche tentare di prevedere qual è la verosimiglianza che un campione ingresso, che è tutto query, rientri o meno in quel modello. Tu puoi fare questa cosa in maniera discriminativa, nel senso che tu cerchi di tirare una linea in mezzo a una serie di dati e cerchi di fare in modo che tutto quello che cada da una parte della linea sia classificato in un modo, tutto quello che caschi da un'altra parte sia classificato nell'altro modo, come di spam contro non spam. Oppure puoi farlo in maniera generativa. Un modello generativo è un modello che stima proprio una densità di probabilità. Quindi praticamente tu stimi quanto è facile che... a che cosa somiglia un campione di tipo spam o di tipo non spam e quindi tu sei in grado di far capire alla macchina com'è fatto una cosa spam e riconoscerlo, ma effettivamente potresti anche utilizzare la macchina per generare nuovi tipi di spam, che è un po' effettivamente la parola generativa, che si utilizza oggi come si dicono generative AI, che serve a dire "facciamo le foto, facciamo gli audio, facciamo la sintesi vocale, cioè". Però comunque tutti questi modelli qua, questo tipo di sottoclassificazioni che ti ho fornito, Quando tu gli passi i dati e gli dici "guarda, il primo dato è della categoria A, il secondo dato è della categoria B, eccetera eccetera eccetera", questo si dice apprendimento supervisionato, perché tu gli stai dicendo "guarda che questa cosa è questa e quest'altra cosa è quest'altra". Le cosiddette etichette, il label, questa è una X, è il tuo campione che entra in questa funzione matematica, e la roba in uscita è il mio aspetto, la Y, è questa qua e io che è già teatrico, quindi tu cerca di imparare quando una X si abitua una Y. Questo è l'apprendimento supervisionato. Tipicamente è quello che si utilizza per costruire modelli, per fare qualunque cosa. La maggior parte del machine learning si basa su un apprendimento supervisionato. Richiede un dataset, tu lo metti insieme, i dati possono essere più o meno disponibili. Chiaramente costa sempre molto creare un dataset supervisionato, questo è un po' il limite dell'approccio. Cioè tu vuoi classificare foto di cane e gatti, hai bisogno di un cristiano che si metta lì a dire "Questo è un cane, questo è un gatto, questo è un cane, questo è un gatto" per un milione di volte e poi lo passi al modello che capisce cosa è un cane e cosa è un gatto. A quel punto conta l'algoritmo che sa fare il migliore uso dei dati. Dall'altra parte ci sono i modelli non supervisionati. Il modello non supervisionato è quando tu passi un insieme di dati che non hanno un'etichetta. E allora perché dovresti fare una cosa del genere? Perché anche senza sapere che quel tipo di immagine è un cane e quel tipo di immagini è un gatto, la macchina è tranquillamente in grado di separare tutte le immagini che sono cani, separare tutte le immagini che sono gatti, per esempio. Perché lo capisce che ci sono certe immagini che hanno qualcosa in comune e che sono diverse da altre. Tu ti dici "guarda, io non so cosa ho trovato, ma trovo due insiemi di immagini fatte in questo modo e di immagini fatte in quest'altra". e tu dici "ok, facissimo, questi sono tutti insieme di gatti, insieme di cani". L'apprendimento non supervisionato ti permette di fare clasterizzazioni, cioè fare gruppi, spontaneamente definire dei gruppi all'interno di una distribuzione dati. O ti permette anche di fare tassonomie, ti permette di fare moltissime cose. E è estremamente attraente questo modello, perché non servendo un'etichetta, tu puoi prendere tutti i dati che vuoi. le foto vai su Google, scrivi gatti, pensi quante te ne pare, le tiri tutte quante giù, hai fatto tutta la set. E questo cosa dell'apprendimento non supervisionato in realtà serve persino per migliorare le prestazioni dei modelli supervisionati, perché le puoi utilizzare, per esempio, per impostare un modello iniziale che poi affini tramite un procedimento supervisionato. Oppure, se ci sono alcuni tipi di modelli che sono addestrati a riconoscere quando un dato di ingresso viene distorto in un certo modo. E questa cosa si dice self-supervised, in realtà, si dice auto-supervisionato, perché la macchina etichetta da sola i propri dati, perché, per esempio, ci sta un modello che abbiamo fatto a Facebook, a e-research lab, che serve a fare delle firme, delle signature praticamente, degli hash, diciamo, di un'immagine. Tu magari vuoi prendere tutte le immagini di porno, fare tutte queste belle signature, le trasformi in dei vettori numerici, le ficchi in una base di dati di ricerca vettoriale, un elastic search, diciamo, e poi puoi cercare immagini query e vedi se è porno o non porno. Che succede però se io altero il pixel dell'immagine? si altera anche il vettore, no? Quindi tutti questi algoritmi di hashing chiamati "percettuali" falliscono quando tu arricchisci l'immagine con delle diascalie, con delle icone, delle emoji, ci metti sopra, l'immagine si distorce, in questo senso. Oppure la roti di un grado, tiè, hai cambiato i pixel, buonanotte, lascia neanche più lo stesso, lo schiarisci, eccetera. però hanno fatto un modello di machine learning come rete neurale che è un algoritmo di copy detection, cioè di rilevamento delle copie tu prendi un'immagine, la arricchisci, la distorci quanto ti pare lui riesce comunque a capire che è sempre la stessa immagine con solo delle cose sopra perché impara a ignorare tutti questi particolari aggiunti come hanno fatto? hanno preso un'immagine, l'hanno arricchita di schifezze e poi gli hanno detto "sappi che queste due immagini sono la stessa entiera cosa" sappilo, è così, capiscilo e l'aerite neurale ha imparato a distinguere quando un'immagine in realtà è un'altra e questa cosa si utilizza perché tu mandi l'immagine porno che però c'ha aggiunto delle emoji qui e là, c'ha aggiunto delle scritte c'ha messo magari una bella cornice, un bordo nero interiore che cambia tutti i bit dell'immagine però l'aerite neurale non si fa fregare, come il nostro cervello non si farebbe fregare e capisce che è un'immagine pulita, in chiaro, che sta dentro la base dati e quindi, zac, riesce a beccarla. Questo è un tipo di apprendimento self-supervised e gli apprendimenti self-supervised sono potentissimi perché ti permettono di avere i benefici di un apprendimento supervisionato quindi puoi fare classificazione, puoi fare regressione, tante cose ma hai la bontà di avere gozziliardi di dati con cui puoi creare modelli estremamente più performanti di quelli che potresti con un dataset etichettato. Questo diciamo è la cosa. Mentre parlavi pensavo una cosa che non è chiarissima nella mia mente. Io come da premessa ci capisco una cipa. Quando ci troviamo nel dominio del machine learning e quando invece ci troviamo nel dominio del deep learning? Perché questa cosa non mi è mai stata troppo Allora, l'idea è che bisogna dare un sguardo all'indietro. La i è nata tanti anni fa, nel 1970, ma anche prima della verità. L'idea è che all'inizio ci siamo messi in testa che con una serie di regole avrebbero potuto insegnare una macchina a essere furba nel approcciare un problema e quindi perbenere una soluzione. Poi sono resi conto che, questi sono i sistemi rule based, effettivamente, poi sono resi conto che dall'osservazione della distribuzione di dati sarebbe stato possibile, effettivamente, capire tutte le sfumature tra i vari tipi di situazioni che la macchina sarebbe trovata da affrontare. L'idea, quindi, di osservare tanti esempi di situazioni, di dati, e costruire un modello che capisca come portarsi in vari tipi di esempi, più che la natura è estremamente sfumata, è in grado di creare tanti esempi della stessa entità di cose che sono diversissimi. Pensa un pochettino a noi che siamo esseri umani, abbiamo tutti facce diverse, ma siamo tutti quanti in realtà persone. E una macchina per capire che una persona, piuttosto che un cane, non gli può far vedere tutte le facce possibili e immaginabili. Devi creare una presentazione mentale di quali sono i tratti comuni. E il machine learning si occupa di questo. si occupa con tecniche che sono statistiche, effettivamente, di aiutare a creare dei modelli matematici di quelle che sono le distribuzioni di probabilità dei dati. E questa è una delle cose più terrificanti da dire a un ragazzetto che si approccia alla materia. Dice "Guarda, tu pensi che il machine learning sia la cosa più sexy del mondo, non c'è niente di sexy nel machine learning. È matematica. È solo matematica. e se tu odi la matematica tu non puoi fare machine learning. Zero. Zero. Zero. Perché è impossibile, non capiesti quello che succede. Bisogna amare la probabilità, la statistica, per capire ben bene quello che succede nel machine learning. Ad un certo punto qualcuno si è reso conto che queste benedette funzioni matematiche, che non facevano altro che mappare da un input ad un output, tipo Y=f(x) e proprio così, potevano funzionare con un formalismo che è quello del "faccio una trasformazione su un input" quindi magari moltiplico il mio input per due poi prendo e faccio una trasformazione matematica ne faccio un'altra, un'altra, un'altra fino a che l'uscita, in un certo senso, non è quella che mi aspetto io a livello di dati e questa serie di funzioni matematiche fanno una serie di trasformazioni in cui tu ci metti in mezzo una trasformazione che noi chiamiamo non lineare per evitare, è una cosa matematica, per evitare che tutto quanto collassi in un unico calcolone ma tu in realtà vuoi tenere i calcoli separati questo tipo di formalismo si chiama rete neurale una rete neurale, in effetti, non è nient'altro che una serie di funzioni matematiche lineari che sono messi a sandwich con una serie di altre funzioni matematiche che si chiamano non lineari e le funzioni lineari e non lineari hanno questa bellissima probabilità che non si possono moltiplicare fra di loro per fare una funzione unica mentre tu puoi fare, prendi una cosa, la moltiplichi per due, ok, l'uscita la moltiplichi per tre in teoria è lo stesso risultato se la moltiplicassi per sei direttamente, no? il per due e per tre possono collassare su un'unica funzione Metti che tu in realtà vuoi tenerle separate, queste per due, per te, perché da più espressività alla tua funzione, cioè ti permette di cogliere una serie di passaggi intermedi che ti servono. Allora tu che fai? Ci metti in mezzo una funzione non lineare, che è moltiplicata per due. Poi, se questa cosa è superiore a... il risultato è superiore a 5, allora moltiplichi per 3. fai che questa tua cosa, se non è superiore a 5, è uguale a 1, oppure uguale a 0. E lo moltiplichi per te. E questo lo definisce l'uomo a priori o è conseguenza del rapporto ingresso-uscita? Allora, il fatto che io faccio una moltiplicazione, poi c'è questa "if" in mezzo, e poi un'altra moltiplicazione la definisce l'uomo. Questa è l'architettura della rete negrale. E' vero, le rete neurali sono delle classi di funzioni matematiche strutturate in questo modo. Poi noi decidiamo qual è l'ordine delle funzioni e come le mandiamo a mettere a sandwich con non-linearità e queste cose sono i cosiddetti neuroni, i cosiddetti strati della rete neurale. E il machine learning engineer studiava una volta una serie di architetture che permettevano di giungere a delle conclusioni. Poi quello che la rete fa da sola è apprendere per cosa deve fare le moltiplicazioni, tipo devi moltiplicarlo per te e per due e perché non per quattro e non per uno, questo lo decide la rete. Poi sono i pesi che stanno all'interno delle funzioni matematiche. In un certo senso questo è il deep learning. Ok, dimmi se ho capito, faccio uno step back perché qua i giochi si fanno un pochino interessanti e anche un po' complicati. Dimmi se ho capito bene. Io ho un ingresso e un'uscita aspettata, giusto? - Sì - Attesa E ho un'architettura di rete neurale L'architettura viene fatta dall'architetto della situazione che poi ti chiederò quali sono generalmente le cose che l'architetto guarda per pensare a una rete neurale - Sì, se non passava il mato quello ti avevo chiamato - Guarda, te lo dico da qua A questo punto praticamente l'algoritmo definisce i pesi in autonomia. Esattamente perché praticamente quello che si fa è tu infili dentro un esempio, un input, lato X, esce fuori e la rete neurale partorisce un'uscita. Tu devi immaginare che i pesi all'inizio della rete sono tutti quanti messi completamente a caso. Non completamente a caso, perché seguono delle regole per evitare delle brutte inizializzazioni. Quindi ci sono delle inizializzazioni che sono migliori di altre. Dei modi di inizializzare i numeri completamente a caso, che però sono migliori del veramente totalmente a caso. Seguiamo delle Gaussian, facciamo in modo che la somma dei numeri sia ben bilanciata in un certo modo, perché è più facile così poi prevenire una configurazione definitiva. Comunque, entra l'input della rete, fa una moltiplicazione. si condiziona sul numero, ha un'uscita. Tu vedi quanto si è discostata l'uscita attesa da quello che la rete ti ha creato e a quel punto dai un metaforico cazzotto in testa a tutte le parti della rete che hanno contribuito di più a quel discostamento. Cioè se la rete spara un casino oltre quello che dovrebbe, tu dici alla rete "devi pompare di meno" è andato troppo in alto rispetto a quello che volevo io. Quindi identifichi la funzione matematica che sta facendo. Sì, ma la parte che lo fa la rete da sola, tu gli dici "pompa di meno" perché "oh, mi aspettavo questo e hai fatto questo" e questo automaticamente si vede. A quel punto di questa differenza, lei partisce equamente terzando i pesi che hanno contribuito a darti quella cosa in più e automaticamente la rete da sola propaga, si dice, l'errore all'indietro, equamente distribuendo tutto il margine di errore e che lo trasforma in una sterzata di pesi. Quindi la prossima volta la rete sparirà un po' di meno. A furia di fare questo con tanti tipi di esempi, centinaia di migliaia, milioni, non so quanti ne servono, dipende dal tuo problema, quanto è ricca la tua rappresentazione, tu infine ti troverai una rete che spara esattamente quello che sarebbe a partire da un input che ti gira un output che ha una discrepanza veramente minima. e questo viene chiamato "back propagation", il tipo di tecnica con cui prendiamo il discostamento e puniamo i pesi nella rete che hanno contribuito, e si fa in base a quello che si dice "discesa del gradiente", quindi il gradiente è la differenza che tu hai in tante direzioni, lo spazio tridimensionale in cui c'è il tuo punto, dove è che si è andato peggio? sulle X, sulle Y, sulle Z allora puniamo tanto sulle Z, puniamo poco sulle X e puniamo di un po' sulle Y e quindi tu lo dovete spostare a tutti i pesi finché la tua uscita, o uno spazio "tide-deal" cotizzato non è esattamente sul punto che vuoi e questo è il modo in cui il Deep Learning funziona molto molto bene Hai parlato di tante direzioni, no? Sì E qua mi viene in mente un'idea, una domanda Nel mondo reale noi abbiamo le caratteristiche dell'elemento che ci servono a dare una definizione su quello che è nel dominio del reale. Colore, altezza... Immagini direi che c'è un oggetto che ha i suoi membri, i suoi field praticamente e il tuo punto dati in realtà effettivamente è la stessa entità. Il tuo data point è una persona, la persona può essere alta 1,80 m, può pesare 80 kg, può essere zoppo, può avere la capacità di saltare due metri in avanti. Tutte queste caratteristiche sono effettivamente dimensioni. Queste dimensioni però devono essere convertite in qualcosa di di di di astratto, di numerico, di meramente matematico. Come avviene questo tipo di conversione? Da un elemento fisico del dominio cioè un colore piuttosto che un'altezza, un peso a un dato di tipo astratto? Allora ogni tanto è naturale quindi per esempio magari c'hai l'altezza, l'altezza è una quantità proprio scalare quindi 180 centimetri piuttosto 160. Questa persona è più alta che questa altra perché c'è più il numero più grande in quella particella. Invece quando hai dei dati tipo categorico, tipo colore, per esempio, lì effettivamente non puoi fare questa conversione anche perché magari non c'è proprio una relazione di precedenza ordinale su rosso piuttosto che blu. Mettiamo da 1960 all'80 c'è una precedenza ordinale, 1960 è di meno del 1980, lo sappiamo. Invece col colore non puoi, e allora devi definire degli input a quel punto c'è un modo di rappresentarlo che per dirne uno una volta c'era la one hot encoding che significa che tu definisci tutti i colori possibili, immaginabili quanti ne hai? 65.000? ne hai 256? ok, allora il tuo colore è un insieme di... è un vettore a 256 celle che sono tutti 1 e 0 e solo uno è a 1 a 1 e tutti gli altri sono 0 quindi è trasformato il colore rosso la categoria rosso su 256 possibili valori che questa categoria ha in un valore numerico in cui 1 è 1 e gli altri sono 0 lo spari così e tutti gli 0 spegneranno tutti i neuroni che reagiscono agli altri colori e quell'1 accenderà solo il neurone che reagisce al rosso e in questo modo tu puoi spegnere e accendere sotto i insiemi di tutti i numeri, di tutte le varie variabili delle tue funzioni matematiche che controllano diversi aspetti che contribuiscono per fare quel calcolo che ti dà l'uscita. Come? Sempre rimanendo... Poi una volta si studiava il fatto dell'architettura, poi ti ho detto che oggi non si fa più perché è vero, è successo nel mondo delle architetture la stessa cosa che è successa nel mondo dei sistemi operativi dopo che è uscito UNIX. Spiegami un po' queste cose. Allora, una volta tu prendevi, arrivava un tizio e diceva "Io ho un problema da risolvere, mi serve una rete reale che faccia questo" Tu "Va bene, ok, allora mi serve una rete che deve prendere un input di questo tipo, trasformare un output di quest'altro tipo e cominciare a ragionare" c'erano delle architetture famose, che ne so, c'era il tipo di... una delle architetture più di successo dell'ultima generazione era la cosiddetta sequence to sequence, cioè un'architettura che permetteva di prendere un input di una certa forma, tradurlo in una serie di passi in sequenza che tu vederi per dare un'impressione di una visione insieme, e poi la rete si era condizionata aveva un certo input, aveva uno stato interiore che riassumeva tutta la scena che aveva visto e la potevi passare a una seconda rete a cascata che era la rete che faceva la prima rete che faceva la codifica, l'encoding aveva uno stato interiore che riassumeva tutta la scena, lo passavi a una seconda rete si chiama decoder, che permetteva di far uscire una cosa anche in un formato potenzialmente diverso dal suo ingresso infatti venivano chiamate reti trasduttive ad esempio, una sequence to sequence veniva utilizzata per prendere un'immagine e far uscire forma di tascaglia dell'immagine. E quindi tu avevi un tipo di cella in grado di leggere l'immagine, che tipicamente erano le celle convolutive. Era un particolare tipo di neurone che era in grado di passare una sorta di pennello sull'immagine e vedere come reagirono i numeri quando il pennello passava sopra e faceva una convoluzione di segnali, praticamente. E otteneva una scena che era una sorta di reazione che l'immagine avrebbe avuto a prendere un pennello di un certo tipo e tossarlo di un altro. E questa scena, questa matrice di numeri, che era lo stato interiore dell'autore teoreale, veniva passata a un altro tipo di cella che era in grado di generare una parola per volta. Tipicamente era una cella ricorrente, si diceva, che era sempre la stessa cella, che però era in grado di srotolarsi e su ogni simile parola, quindi tu magari c'avevi un libro, c'avevi un'immagine di un libro che entrava dentro il pennello prendeva, c'era l'analisi di tutti i bordi, di tutti i contorni, colori, eccetera, eccetera e poi la passava al modello di vocabolario che diceva "Una foto di un libro su un tavolo fine" e la rete si fermava da sola Questo era un tipo di architettura estremamente successo perché permetteva di avere una cella d'ingresso che era un tipo di cella diverso, potenzialmente, da quella che aveva uscita. Poi avevi un sacco di altre variazioni, la capacità di usare diverse celle. Potevi usare delle celle che ti assistevano in mezzo, chiamate attenzione, che permettevano di dire "in realtà, solamente questa parte dell'input mi serve in questo momento, Ma tra tre parole, mi sarà più interessante guardare quest'altra parte dell'input. C'erano quindi reticole. Ogni paper proponeva un'architettura, una più strampata, l'altra... Addirittura qualcuno disse "ma non possiamo fare una rete neurale che ti fogli architetture per noi?" E effettivamente è stata fatta, anche se poi non ha avuto gran successo perché riuscì a fare architetture scemotte. Tutto questo è finito. Tutto questo è finito. Non si fa più. Perché? Perché? Saprai meglio di me che negli anni '70, '80, c'era la ricerca sui sistemi operativi, era un campo di ricerca ipertrofico, cioè c'erano tanti sistemi operativi che venivano fatti ognuno, in realtà era buono per un'occasione diversa. Microsoft magari ancora non ci stava negli anni '70 in quel senso, però avevamo i sistemi che erano buoni per fare real time, avevamo quindi sistemi per le... non so, per i mainframe che erano diversi dai sistemi apparecchiature per casalinghe, eccetera, eccetera. A un certo punto tutta questa bellissima ricerca di sistemi operativi diversi è finita molto violentamente quando è arrivato Unix. Unix ha ucciso il settore della ricerca di sistemi operativi, perché per quello andava bene. Era perfetto, non c'era bisogno di scostarsi più da quel paradigma e tutti i sistemi operativi che sono usciti fuori da quel punto in poi, con la notevole eccezione di Microsoft Windows, in realtà, sono tutti quanti una reincarnazione del primo Unix che è uscito. E comunque anche tutte le altre case produttrici di hardware, tipicamente, che era il loro sistema operativo ah, infine, i sistemi operativi che facevano erano sempre i Unix, no? cioè c'era Silicon Graphics che aveva IRIX c'era IBM che aveva AIX HP aveva HPUX Sun Microsystems aveva Solaris la DEC aveva ULTEX poi c'era una serie di altri Unix tipo Inferno Plan 9, tutta una serie di nomi uno più figo dell'altro poi vado Linux e ammazzavo tutti gli sistemi operativi Unix quindi oggi ci troviamo con un solo sistema operativo. Ce n'è rimasto uno solo. E Windows, sotto sotto, in realtà, siccome è POSIX compliant, in effetti anche Windows onora le interfacce IONIX. Perché è successo? Perché infine, in nome dell'interoperabilità, tu avevi un sistema che, se tu volevi fare un nuovo sistema operativo, dovevi onorare lo standard POSIX e finalmente, anti paletti, che infine il tuo nuovo sistema operativo è effettivamente uno IONIX. non ci poteva niente. Ma Unix era ottimo, si era imposto su tutti gli altri. Era un sistema che uccideva tutti gli altri. Nel mondo retireo reale è successa la stessa identica cosa. È arrivata un'architettura che era talmente tanto buona che ha coppato tutte le altre architetture. E questa architettura ha un nome, un cognome, un autore. È il Transformer di Vasvani. Vasvani è un bravissimo scienziato, nel 2017 è uscito fuori con un'architettura e una rete trasduttiva, una sequence to sequence, con un encoder e un decoder, nel quale non utilizzavano più le celle correnti, non si utilizzano più le celle convolutive, si utilizzavano solo celle di attenzione. Cioè la rete è completamente fatta da un unico tipo di cella che sposta la sua attenzione su parti diverse dell'input contemporaneamente con più teste che hanno più occhi e guardano dappertutto. e la rete impara semplicemente a valutare volta per volta il pezzettino di input che gli serve in quel preciso momento per dire la cosa più intelligente da dire. E questa rete ha sbraccato tutto quello che ci stava. È partita nell'NLP, tutto l'NLP un anno dopo usava solo Transformer, poi anche in questione della generazione audio e video abbiamo utilizzato il Visual Transformer e basta, è finita là praticamente, andava tutto a quel paese. e noi abbiamo una sola rete adesso che ha ucciso completamente il campo di cerca delle architetture sulle reti integrali. Tutto il Deep Learning si fa coi Transformer adesso. Non c'è avanzato altro. Quando parli di celle di attenzione, io provo a immaginarmi la struttura matematica su dati astratti di una cella che sposta il focus da un elemento all'altro, ma in realtà non riesco a immaginarmelo. I tuoi dati, quando entrano, sono dei vettori di numeri. Se hai una persona, hai un vettore a 4 dimensioni, che è altezza, peso, colore di occhi. Se ha meno mazioni fisiche, basta. Sono tutti numeri, altezza, peso, colore di occhi, che avevo visto che era una categoria. E se ha una formazione fisica, può essere un vettore che ti dice e zoppo, gli manca un braccio, gli fa male il ginocchio, è cieco, eccetera eccetera. Questo magari prelebbe un modello che cerca di predire la prestazione di uno sportivo, ad esempio. O di un giocatore di qualunque gioco, ad esempio. Quindi è un vettore. Un vettore può essere moltiplicato da una matrice. Una matrice è intanto una serie di vettori di numeri messi un pi all'altro, fanno la moltiplicazione, ottienono un altro vettore. E quel vettore può essere moltiplicato da un'altra matrice, e ancora e ancora e ancora. Una cella di attenzione non è nient'altro che una matrice che impara a capire quando c'è un certo stato interiore alla rete neurale, che è quello poi che ti determina qual è l'uscita che avrai. Qual era il sottostato del tuo vettore che aveva più probabilità di darti un buon punteggio? l'attenzione è nient'altro che una sottorete neurale, diciamo, che impara a spegnere esattamente parti del tuo input per lasciare accese alcune parti del tuo input che in quel momento contano di più. E la rete questo lo capisce, tu gli dai milioni di cicli di addestramento. Dopo un po' ci arriva la rete, perché è inevitabile che ci arrivi. La rete non fa altro che sterzare da destra a sinistra ogni volta che va troppo sopra o troppo sotto e poi la rete imbrocca. E questo meccanismo, che è molto elegante in realtà, che spegne effettivamente o dà meno importanza, fa anche apprendere più velocemente. Perché nel momento in cui tu dai tanto focus alla cosa sbagliata, la cosa sbagliata verrà penalizzata dalla sterzata dei pesi e le altre invece si muovono e ne hanno poco. Quindi la prossima volta che tu dai focus alla cosa sbagliata, quella cosa sbagliata sarà così piccola che ti farà un errore molto minore. e quindi la rete in realtà prende anche molto velocemente. Quindi l'attenzione è nient'altro che una matrice che serve a accendere e spegnere parti della tua rete per permettere ai numeri di scorrere solo a partire da pochi dati fondamentali. Questo è il modo migliore in cui riesco a spiegartelo. No, stai riuscendo a costruire le immagini nella mia mente, quindi sciacqualo. Meno male. E così immagino anche che stia riuscendo a costruire le immagini nella mente dei nostri ascoltatori. quello che pensavo in realtà è questo tipo di processo serve nella costruzione del modello giusto? questo modello poi può essere... ma anche quando tu lo usi poi... ok e infatti voglio arrivare arrivare proprio là che differenza c'è nel processo di costruzione del modello cioè in cosa si differenzia il processo di costruzione del modello e il processo invece di applicazione del modello? Allora la differenza tra il training e l'inferenza. Il training in effetti non è nient'altro che un'inferenza, cioè quando tu metti una query, osservi una risposta e poi correggi tutti i pesi del tuo modello a seconda di di quanto la risposta è stata lontana da quello che ti aspettavi. Quindi è una qualecona risposta, è un'inferenza in effetti, con sterzata di pesi. Perché se te la vuoi praticamente immaginare un pochettino, immaginati la macchina che si guida da sola, tu infili la macchina in tante parti diverse della strada e la macchina va a sbattere, esce fuori strada, sta ferma, fa tutta una serie di cose, finché la macchina non riesce a capire quali sono le interazioni che in realtà determinavano poi quella risposta tra aspettavi. Quindi sono una serie di sbagli, e ancora sbagli, e ancora sbagli, finché la rete comincia a non... a capire un pochettino di più. E a quel punto tu mi potresti anche chiedere "Ma allora che cosa succede? Il modello migliora lentamente?" Sì, e a volte non lentamente, migliora all'improvviso. Per esempio, se tu fai il modello che impara a parlare come Shakespeare, cioè gli fai vedere tutte le opere di Shakespeare, poi dici "componi un sonetto in stile shakespeariano", dopo magari mille cicli di addestramento, vedi che lui cira tutte le teorema a caso. Poi dopo altri dieci mila giri, vedi che comincia a usare gli spazi, comincia a capire che le le parole che tu vuoi sono una sequenza di caratteri proprio degli spazi che la punteggia dura ogni tanto. Poi continua ad andare avanti, vedi che comincia a generare magari delle... degli agglomerati lettere che sembrano parole di inglese ma non lo sono affatto, e poi dopo un po' comincerà a usare le parole vere che ha osservato e su cui non viene pestato dall'algoritmo del testamento. Questa cosa qua è un tipico esempio di prestazioni che aumentano, ma la mano dell'addestramento va avanti e linearmente con l'addestramento. In realtà la maggior parte delle volte va in linea, la maggior parte delle volte è subito migliore dell'80% e poi all'ultimo 20% ci passa il meglio tempo della vita sua a tentare di aumentare piano piano, fino a che ne aggiunge un asintoto da cui in realtà non si scrive più. A quel punto puoi anche starci un altro migliore di anni, le prestazioni non miglioreranno. Ma per i modelli estremamente complessi, tipo GPT, per esempio, e chat GPT, si sono resi conto di una cosa estremamente affascinante, o come potrebbe dire inquietante, ma in realtà per me affascinante, che la cosa fica di GPT è che è capace di fare tutto quanto senza modifiche di architettura. È una stessa architettura che è capace di fare tanti task diversi. Ebbene, la capacità di svolgere un certo task in maniera buona appare all'improvviso solo con un certo numero di parametri da un certo numero di parametri in poi tu fai una rete piccola, per esempio con, tipo, non so, un milione di parametri i parametri sono i tuoi neuroni, i tuoi numeri i pesi di automatrice, flot 22 bit, 16 bit, così se tu dai un milione di parametri quella rete potrebbe non imparare mai a fare gli assunti oppure a rispondere domande con un miliardo di parametri, all'improvviso, pah! Quella capacità appare dopo un po', perché solamente con un certo numero di neuroni quell'abilità si manifesta. E se tu continui ad aumentare i neuroni, vedi che la rete acquisisce sempre più abilità di botto solo a partire da un certo numero di neuroni in poi. La rete deve essere abbastanza intelligente, deve avere abbastanza spazio per riuscire a capire queste cose. Quindi ogni tanto la capacità della tua rete influenza dall'inizio la tua capacità di arrivare all'obiettivo. Altre volte ci vuole solo più dati e più tempo e perché la rete comunque ha abbastanza parametri per farcela. Quindi se io volessi fare un sistema di natural language e volessi addestrarlo, io ho un'infinità di testi. Come ce li butto dentro? Dipende moltissimo dalla tua rete e da quello che tu vuoi fargli fare. In generale, il mondo adesso, perché come al solito il mondo si assesta su una sola cosa, quella che va bene, no? Avremmo detto che il mondo prima faceva tante architetture, poi si assestava sul transformer, poi dal transformer ci siamo assestati sul GPT e da lì praticamente non ci siamo più schiudati. C'erano altre architetture basate il transformer che ormai... - Perché GPT utilizza il transformer, ma è una versione particolare. - Sì, è un transformer pre-addestrato. Il general pertaining transformer, quello è il GPT. Quella T del GPT è il transformer. Ed è un tipo di transformer che non ha l'encoder, ha solamente il decoder. E quindi è una chiacchiatura di transformer mozzato in due, perché non gli serve praticamente l'encoder. Quello che tu fai praticamente è che gli passi un testo, una parola per volta, e gli dici "guarda che dopo questa parola tu devi imparare a dire quest'altra parola". Quindi se tu hai il libro sul tavolo, gli fa vedere "il" e poi gli dici "sappi che da 'il' mi devi dire 'libro'". Dillo. Se non dice "libro" ti la sotto in testa. Quando "dopo", ok, va bene. "Sappi che dopo il libro mi devi dire 'e'". Quindi mi aspetto che dopo che ti mando il libro, in uscita mi devi dire "è" e tu già lo sai che devi dire "è" perché basta guardare la frase come va, no? Quindi è un tipo di apprendimento non supervisionato, in effetti, perché non devi etichettare niente, hai già i tuoi dati di addestramento, no? Allora, tu dici "ok, il libro è ok" poi dopo che il libro è, tu devi dire "sul" e va avanti così, una parola per volta e il GPT, semplicemente a partire da un prefisso di frase che già ha visto, e potrebbe anche essere quello che ha appena generato, impara a predire la prossima parola, da destra verso destra. E è tutto qua quello che fa il GPT, e qui tu gli fai vedere tutto lo scibile umano, più volte, e a quel punto tu gli puoi fare delle domande, "La razza di cani preferita della regina è..." e la rete completerà la prossima parola dicendo di la razza del cane, che è più probabile che da tutti i dati che ho osservato lui osservi. Nella pratica tu devi semplicemente prendere sto monotesto, sto monocorpus con tutta la roba dentro e sparlo nella procedura di addestramento che già ti prevede... dammi solo il nome del file, ci penso io a snocciolarlo una parola per volta perché ormai abbiamo i framework di addestramento come TensorFlow o PyTorch che sono super carenati nel gestire tutta questa idraulica, queste tubature del "dami un file", "sì, lo so che lo devo aprire, so che devo prendere una parola, so che devo costruire un vocabolario che determina anche la larghezza della rete". E la rete si addestra praticamente a trovare delle relazioni nelle parole anche a molte parole di distanza, questa è la gran piccata del transformer. Il transformer è in grado di capire che relazione c'è fra una parola e un'altra a distanza quattro. magari quindi tra "il" già ti determina il fatto di usare un "e" singolare. Se invece "i" e già sai che è un plurale quindi devi dire "solo". Lui capisce dall'articolo al verbo, se lo dovete parlare in mezzo, sa già che deve attivare il neurone che farà un verbo plurale, già lo so perché ho visto l'articolo. Il GPT apprende in questo modo. Adesso voglio parlare del lato SRI e platform engineer che è in te. Quando bisogna adestrare GPT? Sto esagerando, voglio proprio portare la mente a ragionare a quel livello di scala, grandezza di scala. A proprio livello di infrastruttura e di architettura proprio per questo processo come ci si organizza? Perché mi viene proprio difficile immaginare. Allora la cosa più difficile in questo caso è che se tu hai una quantità di dati mostruosa e hai anche tra l'altro un modello che è mostruoso di per sé, perché mi pare che GPT-3 sia un 500 GB di RAM gli servono per girare e tu non puoi metterli su una macchina sola non ce l'hai una GPU da 500 GB di RAM, manco esiste quindi in realtà tu devi essere capace di spezzettare il tuo modello su più macchine ma anche se fossi capace di spezzettare il tuo modello su più macchine che ogni uno si prende una parte dei parametri tu non puoi usare una macchina soltanto per fare il tuo addestramento perché non finisce più allora tu vuoi sparpagliare il tuo addestramento su più macchine possibili Però a quel punto hai il problema che se tu prendi 100 macchine e dai una parte del dataset a ogni macchina e cominci a addestrare, ti aiuteranno poi 100 modelli diversi. Tu non te ne serve uno solo di modello, uno solo che capitalizzi l'esperienza di tutte. Allora ci sono delle tecniche, dei pattern, per il quale si utilizza una sorta di server dei parametri che si occupa di fare la collezione dei parametri come sono cambiati su tutte le macchine in maniera distribuita, faccia la media di tutti gli scostamenti che sono stati imposti a tutti i parametri e poi li spedisca indietro a tutte le macchine che si ricevono così i nuovi parametri aggiornati. Tipicamente... Quindi condividono l'apprendimento in questo caso. prendimento assolutamente perché tu non vuoi diminuendo l'efficacia però nel contempo no no no senza perdita di efficacia questa è la cosa che dica ok perché quando dici la media io immagino che il media il calcolo matematico che viene fatto tra i parametri non tende ad alterare a allenire un po' la passione della pagina il cavolo media effettivamente che è un inganno. Quello che viene fatto è che tu ogni volta che mandi una query esce uscita, ottieni poi il discostamento ancora una volta dall'output atteso, e quello, quel tuo delta, quella tipo +1 su questa direzione, -5 su quest'altra, +7, quello te lo puoi tenere così, lo scrivi da una parte, non vai a cambiare subito i parametri. Ti tieni questo +5, -3, +7 per questa query è questo. Poi mandi tutto il DAL7 in avanti, quindi ottieni un milione di sterzate, o magari anche una volta ogni mille campioni, tu ti prendi tutte le sterzate, le mandi al parameter server, lui applica tutte le sterzate e ottiene un modello nuovo, e dice "Ok, belli, sappiate dove..." - Si riparte da qua. - Si riparte da qua. Oppure, ancora più furbescamente, dice "Ho fatto tutte queste sterzate, te correggite così, te correggite quelli, in modo tale non ti devo inviare tutto il modellone via rete, che costerebbe un pochettino ma cerco di essere estremamente efficiente nella tecnica. I più bravi a fare questo sulla faccia della terra, o perlomeno sono quelli che se ne vantano di più ad alta voce, sono OpenAI. OpenAI, aspetta un attimino che vado un attimo ad accendere la luce perché non vedo niente, anche se lo so che è solo audio, qui la stanza si sta facendo veramente molto buia. - Tranquillo. - Scusa un attimo. OpenAI sono quelli che sono più in gamba di tutti quanti, hanno pubblicato anche un bellissimo paper che facevano vedere il modo in cui loro avevano aldestrato l'affare. Sto un attimo prendendo. Sì, hanno fatto un articolo che si chiamava "Scaling Kubernetes a 7500 nodi". Loro hanno fatto un cluster Kubernetes da 7500 nodi e poi hanno fatto un bellissimo articolo che è molto concollo qua sulla chat. Sì, lo mettiamo nelle note dell'episodio. Qua ti dicono "allora belli, sapete che succede quando c'è un cluster Kubernetes 7500 nodi che sono volati di perdiabetici perché spesso il volantere ha un sacco di nodi che si scommettono da soli. Per esempio Plunnel non ce la fa più a tenere la rete perché c'hai talmente tanti nodi su una singola subnet che a volte perde la linea. Poi dobbiamo configurare, dobbiamo usare un hardware con delle fibre ottiche, poi abbiamo dovuto pensare che il il modo in cui i pod vengono creati e distrutti, non deve farci perdere partire l'addestramento, e ci sono sempre i pod che vanno giù perché cresciano per una ragione o per un'altra. Come facciamo, quindi, a canterare questo fatto? Mi hanno fatto una media dissertazione di tutto quello che hanno appreso su cosa vuol dire scalare un cluster a 7500 nodi e che tecniche bisogna utilizzare per fare in modo che l'apprendimento di un modello così grosso termini entro la morte entropica dell'universo, praticamente. e qui ci sono considerazioni sistemistiche da fare. Tutti i production engineers che lavorano dentro Meta nella AI infra si pongono problemi di questo tipo e pensano a cose di questo tipo. Come cavolo facciamo a fare in modo che questa roba vada liscia come se fosse su una macchina sola, con la correttezza di una macchina sola, ma col parallelismo di 10.000 nodi magari, perché è quello che vogliamo. Esatto abbiamo parlato di apprendimento quindi di fase di training se si può chiamare così ma per quanto riguarda invece l'esecuzione è la stessa cosa e ha bisogno di stesse dimensioni di risorse per l'esecuzione? se l'idea è che tu vuoi fare la stessa cosa che fare in training, cioè fare una query, ma poi non è che ti metti a misurare la differenza, fare un risultato atteso e l'uscita, ti becchi l'uscita buona com'è e la mandi indietro al client, a quel punto bisogna vedere come questo modello tu l'hai fatto, cos'è che il framework che hai usato per addestrare questo ti richiede, per esempio il framework potrebbe dire "prima di avviarmi devo caricare tutto il modello in RAM, perché a questo io so fa" "Oh madonna, quanto è il modello? 100-150GB?" "No, mi sembra 500GB di RAM" o può far bacco. Oppure il modello è capace di fare a fette la matrice di parametri e caricare solo quello che gli serve on demand? La maggior parte delle volte no. La maggior parte delle volte tu puoi testare un modello e ti deve entrare in RAM, anche perché perché se no non affitta più e la maggior parte delle volte deve essere pure la RAM della GPU perché è estremamente più efficiente utilizzare una GPU per ragioni che se vuoi posso anche attaccarti a spiegarteli o piuttosto che utilizzare la CPU del tuo computer e in questo caso quindi c'è sempre uno slicing anche in fase di utilizzo del modello? beh sì perché la maggior parte delle volte il modello pretende di essere caricato in memoria e quindi se tu hai abbastanza RAM distribuita su più macchine il framework ti da una mano e lo fa lo slicing apposto va bene così ci penso io se non c'è RAM ti schioppa in crash in fase di avvio quando mandi la GUI dentro quello comincia a leggere il modello dal tuo disco lo comincia a pompare dentro la memoria finisce la memoria, hai una top memory BOOM! ti trovi lì praticamente ok "eh, non so neanche se è il modello, grazie" - e intanto ti paghi la bolletta cloud - sì, sì, sì, per esempio però allora tu lo puoi mettere dentro la CPU in realtà spesso e volentieri ci stanno dei modi per usare delle GPU più economiche non per fare l'addestramento ma per fare la query fare l'inferenza ti permette di utilizzare dei processori più economici rispetto a fare tutto il processo di addestramento tu puoi usare degli acceleratori solo per le query e il caso dei processori mi sembra solamente da inferenza su amazon ci sono delle istanze che sono fatte solamente per fare inferenza le istanze A se non mi sbaglio che erano quelle del processore graviton ho letto qualcosa ma... però esiste hardware più economico una volta un tizio ha voluto presentare una pendrive che conteneva un processore per fare l'inferenza accelerata quindi tu utilizzavi il canale usb per caricare il modello solo in inferenza se non il addestramento. Quindi potevi utilizzare un super computer per fare l'addestramento e poi la tua batteria da un milione di macchie e le dati di uno GPU più scarsa, ma che comunque permette di fare il calco in hardware invece che in software. E anche se non c'è tutta 'sta RAM, in realtà utilizzi la RAM di sistema, ma le moltiplicazioni le fai su questa pennetta a parte. È un coprocessore matematico per fare le moltiplicazioni di matrici in hardware e costa tanto di meno, ovviamente. mentre parlavi di inferenza e di modelli che funzionano bene alla inferenza ho pensato per un attimo, magari mi sbaglio, a OpenAI Whisper Function as a Service? No, OpenAI Whisper è il speech to text Ok d'accordo, non sono sul pezzissimo in questo senso. Io sono imbarazzante in questo perché un sacco di cose, in realtà un sacco di aggeggi non li utilizzo perché mi piace studiarli ma non mi piace usarli. No, adesso ti spiego perché dove voglio arrivare però. Noi usiamo OpenAPI Whisper per fare il text to speech di lo speech to text di tutti gli episodi di github. Ah certo. E utilizziamo il modello large che non mi ricordo penso sia un po di giga e io lo faccio girare sulla gpu sulla mia scheda grafica da gaming è una 3060. Cos'è una? 3060. un bel pezzo di hardware che c'hai. E' un po' vecchiotta, c'ha un paio d'anni. Però si funziona bene e devo dire che un'ora e mezza di episodio ce la trascrive in 15 minuti ed è una figata pazzesca con una qualità che nessun provider ti dà. Nessuno, io le ho provate tutte, ho provato lo speech to text le funzioni di transcribe di AWS quelle di Microsoft quelle di Microsoft le ho provate un po' di tempo fa non so se adesso stanno utilizzando le API di Whisper di OpenAI e questa cosa funziona figa perché lui si scarica il modello e semplicemente con tre righe di Python io faccio la trascrizione delle versioni. Sì, ottimo. Le caratteristiche sono sulla pagina di OpenAI Research Whisper. Sì, è un transformer che prende degli spettrogrammi in bandanella e li passa su un microcomputer. Ecco, volevo arrivare proprio a quello. Lui si scarica questo modello di 20 Ma poi come fa a prendere dell'audio, che per me sono onde campionate, e trasformarlo in parole? Allora quello che lui fa, lo stiamo spiegando qui, praticamente è un encoder decoder, quindi vuol dire che è una rete sequence to sequence, cioè lui prende una trasduttiva, cioè prende prende un input in un formato, che è il tuo audio, e lo tira fuori in parole. Quindi deve fare una modellazione del tuo ingresso in uno stato interno e poi utilizza quello stato interno per condizionare la rete di uscita a emettere le parole che vuoi tu. Il modo in cui prende l'audio e lo trasforma in informazione su cui lui ragiona lo fa perché fa un'analisi spettrale, cioè lo fa nel dominio della frequenza, fa una trasformazione di Fourier e lo passa dal dominio del tempo, la tonda audio, la waveform che fa così, lo trasforma nel dominio della frequenza, quindi lo trasforma in una matrice nel quale tu vedi tutti i valori di tutte le frequenze della banda audio che va da 0 a 2 kHz e sono delle matrici di numeri che per ogni secondo ti dicono, per tutta una gamma di frequenze, che valori c'erano a quella frequenza in quel secondo. Questa cosa si chiama spettrogramma. E lo fa in una banda particolare, sono due bande, la mia conoscenza da ingegnere audio è limitata, però so che sono due bande. Perché ho lavorato a implementazione del primo speech to text chiamato Tacotron, l'ho è un programmato di implementazione con framework di Amazon, che si chiama Mixnet. Quindi so più o meno di cosa parlo. Anche se la tecnologia che abbiamo usato qua è diversa, perché il tema è venuto dopo ieri. L'idea è che tu prendi, e quando usi, ci sono tante librerie che fanno questa roba per te, Rosa, per esempio, una libreria in Python che ti permette di prendere audio, mp3, wave, e lo suddividi in bande. la banda lineare che è la banda praticamente normale in varie frequenze del quale tu ogni cella c'è una differenza, una differente frequenza per ogni tipo di secondo quindi tu hai i tuoi campioni, hai 44.100 campioni al secondo, no? il classico 44.1 kilohertz, una parità tu hai 44.000 matrici al secondo e ogni matrice contiene una serie di numeri su una banda di frequenza. Questi sono tutti i campioni che tu hai. Se tu hai un audio di più bassa qualità, di 22 kHz, hai 22.000 mappe di circo ogni secondo. La banda MEL è una banda che imita su 80 bande possibili, e imita il funzionamento del modo in cui l'orecchio umano percepisce l'audio, quindi enfatizza certe frequenze più di altre. L'idea è che tu dalla banda MEL, quindi di questa serie di spettrogrammi di matrici che ti dicono in questo preciso campione i dati sono questi vedo che qua c'è un grafico scritto "conv1d + gelu" conv1d significa che fa praticamente una passata di una rete convolutiva qui passa il pennello che praticamente cerca di stabilire i rapporti fra un segnale e un segnale che gli è a una certa distanza, secondo la larghezza del pennello. Magari è un pennello che è largo tre, quindi becca un segnale e i tre segnali successivi. che cerca di stabilire una correlazione fra un campione e i tre campioni successivi, ma è l'argo 5 di solito, eccetera eccetera. Lui cerca di passare questo pennello, cerca di fare una media di un segnale con i cinque segnali che gli stanno appresso, per esempio. Dopodiché viene fatta passare da una GALU, una Gaussian Error Linear Unit, cioè è un neurone che spara solamente se il segnale è positivo, altrimenti dice zero, ma con una regola di morbidezza che dice se il segnale in realtà è al di sotto di negativo, non mi dice zero. Dimmi un numero che in realtà è compreso fra zero e meno uno e arrivoci con una curva dolce che imita una distribuzione gaussiana. - Stiamo andando a Super Room! - C'è una proprietà che è molto più dolce nel modo in cui la propagazione di errore viene fatta, perché magari tu hai una serie di numeri negativi, si sformano tutti in zero e tutte quelle che erano muoiono, perché non apprendono più niente, quindi qua lo scostamento solo cosa per zero, zero, no? Quindi lo finisci e tutti gli errori ti crepano perché hai avuto tutti i segnali negativi e vanno tutti a zero, quindi una parte del tuo cervello smette di funzionare in quel e non è bello poi quello che vedo che fa, ci mette una sinusoide si che è un pet che aggiunge una marcatura temporale cioè lui per capire che una cosa viene prima di un'altra ci prende una bomboletta spray e ci disegna una sinusoide sopra così vede che i pezzettini se quando sono vicini la linea è intolce, vuol dire che erano un appresso all'altro e quindi uno viene prima dell'altro -Figo! -Si è un truccone, vedi che sono inventato il Transform per fare una marcatura temporale di una serie di campioni. Poi vedo che c'è "encoder, encoder, encoder, encoder" quindi fa una serie di passaggi con un transformer liscio e lo fa probabilmente chissà per...ti blocchi forse la profondità di solito è 16, quindi probabilmente sarà quello. E poi il decoder, quello che fa è quando tu lo stai addestrando, che hai già alle scritte praticamente tu lo fai addestramento, quindi magari hai un libro e la voce che narra quel libro, per esempio. Sai qual è il libro migliore del mondo per fare questo tipo di addestramenti? Non lo diresti mai. La Bibbia. La Bibbia è il libro più fantastico del mondo. Perché? Perché la Bibbia è... Uno, è annotata. La Bibbia ha tutti i versetti, quindi tu puoi annotare ogni singolo subbrano e avere un addestramento estremamente preciso di cosa stai dicendo a quel punto dell'audio. Non c'è proprio il rischio che tu vada più veloce o più lento e quindi praticamente in realtà non sai a che punto della frase sei. Ha delle frasi estremamente brevi e annotate bene e soprattutto che sono anche multilingue, quindi quel versetto in quella parte sai cosa corrisponde al versetto di un'altra lingua. Bibbia, pavolosa, è un dataset di addestramento che a parte il trebbio di sueto è buona. Sai qual è? e tra l'altro lo trovi anche recitata, quindi ci puoi addestrare gli speech to text. Invece se ti interessa solamente fare traduzione in multilingua, una cosa che si utilizzava spesso, perché era open source e quindi si trovava, obbviamente open source comunque, quindi nessuno dice niente se la usi, erano le minute delle riunioni del Parlamento europeo, i procedimenti parlamentari, perché sono tradotti in tutte le lingue dell'Unione europea. e quindi tu hai dei corpus multilingua pubblici, perché è pubblico, a gratis è vero, non ce l'avevo pensato questa era l'arte di progressività, dove diventare molto creativo per trovare dei dataset quando uno non ce l'aveva e allora c'erano tutta una serie di siti che facevano crowdsource di dataset anche allineati non sempre bene, però non importa perché quando c'era un errore nella media si correggeva, c'era un enorme sito che si chiamava Opus e Opus è la biblioteca dell'Essandea dei dataset multilingua, cioè tipo open subtitle, cioè i sublimini di un film allineati. Ottimo, lì c'è il minuto X che si stanno a dire. In realtà ho visto che c'era... non so da cosa dipenda, ma Open Whisper ha un po' di shift talvolta sul timing che ti dà delle trascrizioni, perché tra l'altro ti fa anche i marker temporali per ogni... quello perché Popeino ci deve pensare quindi non è in tempo reale cioè la computazione non riesce a stare al passo con il dialogo no no no ma parlo anche della... io gli carico un file e lui fa la trascrizione ho notato che ci sono delle imprecisioni però credimi il modello è incredibile se penso che mi giri al locale sul mio computer e tra l'altro... un computer di casa. La democratizzazione dell'hardware che permette a chiunque nel proprio garage di farsi la sua ricerca o comunque l'applicazione del suo modello è chiaramente una delle conquiste che abbiamo avuto di recente. La IE diventava democratica e per questo c'è stato il rinascimento dal 2014 in poi perché tutti riuscivano a far girare un modello sul computer di casa e questa cosa è successa grazie a NVIDIA che ha creato CUDA perché qualcuno si è detto scusa ma tutte queste scene di questi videogiochi, questi oggetti 3D che io vado a far girare che effettivamente sono tutti calcoli matematici, ogni volta che fai una rotazione di un modello, il personaggio tuo si volta nel gioco o cambia delle camere a dio non voglia devi rotare tutto il mondo, devi applicare trasformazioni matiche a tutti gli oggetti oggetti da scene, i poligoni sono tanti, sono centinaia di migliaia, o milioni anzi, sono già centinaia di migliaia nel 1998, l'immagine vista davanti ai poligoni, sposta adesso. L'idea è che, questo disse, ma invece devi ruotare un oggetto 3D che non me ne frega niente, ma io perché non mando delle matrici di dati che se le trasformi in poligoni non vogliono dire assolutamente niente, proprio gue alla cazzo di cane. Però io posso prendere una matrice, ci applico una rotazione sopra, o ci applico una trasformazione con un'altra matrice, che nel mondo dei videogiochi solitamente è una rotazione. Nel mio caso è una matrice che dico io, e qui non deve fregare niente di niente. Faccio questa moltiplicazione, la faccio in hardware, e mi annoto il risultato. Ma allora io posso utilizzare la GPU come coprocessore per calcoli matematici. E quello è stato l'inizio. La prima cosa che la gente ha pensato a fare è fare Brute Force in DmD 5. La prima proprio. E poi qualcuno ha detto "Aho, ma io posso farci anche calcolo su reti neurali? Viva la rivoluzione!" E in realtà le GPU effettivamente riescono a surclassare i processori dei computer, perché le GPU sono macchine estremamente parallele. C'è la differenza che c'è fra una Ferrari, e la Ferrari è il processore del tuo computer, che è una macchina velocissima, ma seriale, e un Camion, che è una macchina più lenta della Ferrari, ma che di pacchi ne porta tanti, tanti di più, tutti insieme. E il segreto della velocità del GPU è la banda passante che c'è a bordo della GPU stessa, che è spettacolare. Cioè tu magari hai una banda di 15-18 GB al secondo sul Northbridge, tra il lato CPU e la memoria RAM, ma a bordo della GPU, la sua memoria locale, con i suoi Tensor Core, ci sono 935 GB. La scheda che hai tu, la tua RTX 3060, dovrebbe avere tipo 900 GB al secondo di banda passante a bordo della GPU stessa. E il segreto della velocità è che tutte le Tensor Unit sono in grado di accedere contemporaneamente, in maniera indipendente alle sottoporzioni della ram del tuo file, non uno per volta, come fa il processore. Se in realtà vuoi accedere a un indizio, è quello, e poi torni indietro e avanti col prossimo. Invece tutti i TensorCore si comportano indipendentemente, e tra l'altro ogni TensorCore può fare scheduling di un gruppo di thread, tipo fino a 1024 thread, mi sembra, col minimo che è 32, che si chiamano warp, si chiamano nel linguaggio del framework di CUDA, e tu puoi fare lo scheduling di questi warp ogni volta per accedere a porzioni di memoria che addirittura nella serie RTX della 30, con i Tensor Core, quelli veri, tu non leggi neanche indizi di memoria, tu leggi mattonelle di memoria, chiamate tile, perché tu un'immagine la vai a provare a mappare in una porzione di memoria che è quadrata. Quindi va bene che la RAM sia ottimizzata per leggere non un indizio per volta, ma sottoporzioni, sottomateci. tutte contemporaneamente in unica passata e questo è il segreto della velocità GPU c'erano degli articoli fichissimi tipo di "Deathmird" "Team Deathmird" ha fatto un articolo stra fichissimo che spiega come... perché l'hardware spacca da GPU per fare queste cose - Io guardarei marrei settimane ad ascoltarti quando guardi queste cose qui quindi zappa ti faccio l'ultima domanda vedo che siamo già un'ora e mezzo, noi ci riproponiamo sempre di stare dentro l'ora, ma stai tranquillo, sotto l'ora e mezzo... - Ma avevo detto che io posso attaccare la conferenza su qualunque cosa! - Ma stai sicuro che... - Mi fomento come un cretino ogni volta che parlo di queste cose, perché veramente... - No, beh, sono... - Non mi si dizzano i capelli in testa per quanto sono ben fatte e quanto sono ben pensate e efficienti. Sono tecnicamente affascinanti però adesso arriva il momento della domanda bastardella. Perché in realtà tutto il mondo dell'intelligenza artificiale è super figo per noi tecnici e geek. Esiste un lato però che sta iniziando a nascere e dove ci si sta fermando un attimo e si sta ragionando sull'impatto e sull'etica. Volevo farti una domanda, come vedi tu la responsabilità e che impronta etica secondo te bisogna dare a questa vera rivoluzione di rivoluzione strata? Beh allora il fatto che esista una etica è una cosa dia perché il modello che tu a destri, recita quello che vede dai dati. Quindi, se tu prendi dei dati che, pur essendo giusti, sono un riflesso di una realtà, una realtà che magari è ingiusta, otterrai un modello che fa cose ingiuste e dice cose ingiuste. Due esempi. Il fatto che l'80% dei carcerati in America sono neri. E questo perché riflette, non perché i neri siano delinquenti, ma perché riflette il fatto che le fasce più povere della popolazione, nell'80% dei casi, sono nei. E' un squilibrio che hanno la società in Stati Uniti. Qualcuno ha pensato bene, facciamo un modo di machine learning che provveda qual è la possibilità di recidiva di una persona a partire dai tipi di errati che ha fatto e lo utilizziamo per decidere la libertà, condizionare la libertà di gida, altrimenti concederla o meno. e saltava fuori che siccome l'80% dei carcerati sono neri, i neri sono quelli che sono più facili che li facciano reati e quindi no, la libertà vigilata sei nero e questa cosa è uscita fuori, l'hanno messo in produzione, l'hanno osservato e si sono accorti che faceva così e qualcuno ha detto "no ragazzi, avete creato un modello che è razzista" perché è un modello che parte da delle assunzioni che sono quelle di un bianco, cioè che tutte le persone sono uguali per nascita, manco per il cavolo, se sei nero probabilmente sei cresciuto in un ambiente che è povero, quindi sei molto più esposto a uno stile di vita che potrebbe portarti a diventare un delinquente. Oppure il fatto che tu non puoi fare a partire dal colore della pelle, vai a giudicare se questo può essere un delinquente o meno. No, non è solamente ingiusto fare così. Un altro pure era il GPT-3, che era un modello fichissimo, ma addestrato su un sacco di dati, con un sacco di parametri, però diceva cose tossiche. Tu gli chiedevi la parola... se io dico la professione professore, la persona che ha il titolo di professore, secondo te è uomo o donna? Diceva uomo, ovviamente. Se io dico genio, e non è come l'italiano che tu devi per forza declinarla al maschio e femmile, genius non è neutro in inglese, no? Genius è uomo e donna, dice uomo ovviamente, oppure dicevano cose razziste, dicevano cose sull'in... Cioè, GPT è un modello che ha ottimizzato per avere bocchetta di fiato. L'importante è che fosse ben formato quello che diceva. non ha importanza di quello che diceva fosse una schifezza a meno e quindi loro crearono un modello, un esperimento chiamato InstructGPT che serviva con l'input umano a stersare i parametri del GPT-3 in modo che fosse d'aiuto, che non fosse tossico, che fosse... sì, amichevole nei confronti delle persone che hanno inventato ChatGPT in questo modo è nato così, è nato per un problema di bias è un problema di etica, perché non abbiamo permesso alla macchina di dire tutto quello che gli va a fare, no la macchina ha sempre ragione, ha tutta la conoscenza del mondo, ma la conoscenza del mondo è scritta da bianchi di destra e quindi che pretendi? La macchina si comporterà come un bianco di destra, no? Lo sto molto banalizzando, ma è questo che in effetti è successo poi. Riguardo al fatto che l'AI non ce la dovrebbero dire tutti, è quello che dice Elon Musk, soprattutto quello che dice adesso Penai Penai adesso gli dice "l'AI non è per tutti" "noi non possiamo aprire il nostro modello e non possiamo darlo a tutti" "perchè la gente si nervosa per farci le schifezze" infatti, GPT-3 è stato l'ultimo modello ad essere aperto GPT-4 è già closed, non si sa non te lo danno proprio, se non tramite API e anche chat GPT non te lo danno e dietro un paywall, perché non vogliono che la gente utilizzi questa tecnologia per farcela qualunque, perché è veloce, si può creare lo spam e comunque avrò perso un sacco di vantaggio competitivo. E' stata una forza che ha determinato un camminuto positivo, perché ha alzato l'asticella del livello di competizione e ha fatto apprendere la scienza del parecchio, così come DeepMind, che fa solo tre ore di gioco. Sulla democratizzazione sono assolutamente a favore, non esiste la cosa, deve essere tenuta segreta perché non rende il mondo un po' più sicuro. Sul bias, ci sono a favore del fatto che dobbiamo stare attenti a non fare in modo che le macchie non mimino i nostri vizi e si compartiamo con noi. L'unica cosa in realtà che mi preoccupa nel senso più largo del termine sta nel fatto che questi strumenti iniziano a dimostrare la loro potenza e la loro influenza nel mondo e quello che mi chiedo è una corporation privata che ha degli strumenti così potenti in esclusiva può essere un problema se ragioniamo. Ci hanno messo soldi, OpenAI ha pagato Fior di Scienziati, Fior di Florente, Fior di Hardware. Lo so Alberto, quello che mi chiedo io è però verissimo, in questa condizione attuale è normale che sia così e loro hanno tutto il diritto di reclamare questo, però noi pensiamo che ci sono anche tutta un'altra serie di industrie che sono fortemente normate, dove cose che hanno impatto così globale devono sottostare ai controlli, delle regole, immagino il mondo dell'avionica o dell'aeronautica o il mondo degli armamenti militari. Esatto, sto per dire le pistole, le pistole insomma. E sono fortemente normate, fortemente vincolate. Sono a parte, io parlo da geek, ma è nel contempo da cittadino, no? E dico, ma cazzo, se noi stringiamo i lacci del controllo, mettiamo un tappo all'innovazione. Perché oggi il controllo pubblico non è in grado di fare innovazione. Se prima la ricerca era pubblica, oggi non è più così e non può essere così. è un dato di fatto, dobbiamo esserne consapevoli. Ci devi mettere il terzo giocatore. Scusami? Ci devi mettere il terzo giocatore. Pubblico, privato, community. Perché il pubblico è ciò che fa lo Stato, il privato è ciò che fa l'azienda e poi c'è quello che fa la community. La folla su internet che si riunisce e fa i modelli. E la maggior parte dei modelli copycat che escono fuori oggi, cioè tutti i vari gpt aperti tutti i vari chat gpt aperti sono community e lei non c'è proprio modo di fare un grand cave perché non poi c'è la stesso addestramento poi viene rilasciato in community il codice rilasciato in community uno può reimplementarsi chat gpt a casa propria a quel punto poi lo rilasci e non è che c'è bisogno di una licenza per farlo è difficile regolamentare la community in questo senso. Cavo, ci siamo riusciti con la pirateria musicale! Che c'è? Vero, però io sono stronzo nell'alba. Vai, vai, vai! È anche vero che se noi dovessimo fare una tara sulla community potremmo anche dire che OpenAI era community quando è nato. OpenAI è community però. Come imprinting! Domanda, la community a me sembrava una no profit, cioè una compagnia privata che poi rilasciava tutto quello che faceva. Però se noi guardiamo su Github, guardiamo su Github ed è una domanda che mi sto facendo anche nel mondo dell'open source, no? Sì. Quello che è open source oggi ed è open source che funziona sotto a una o più società che hanno, di una società, quella per cui lavoro, che fa tantissimo open source, ma è sempre comunque una corporation che ha interessi a condividere e il prodotto grande che noi chiamiamo community open source viene da queste società che condividono dividendo l'effort. Lo vedo un po' molto diverso da il concetto di free software di 30 anni fa, capisci? Ma OpenAI ha avuto un sacco di contribuzioni pubbliche? Cioè su TTAB c'era gente che partecipava alla roba di OpenAI? Secondo me si sono fatti tutto in casa. Questo non lo so, ti dico la verità. Secondo me si sono fatti tutto a casuccio loro, non hanno avuto... non lo so, per dir la verità, non mi sono mai andato a studiare i loro repository, questo è un po' anche il mio viziaccio di non voler utilizzare gli strumenti perché non mi interessa più studiarli che poi usarli, quindi in realtà questo mi tiene un po' a distanza dai repository di Github, però so che Tensorflow ha avuto una serie di contribution, se non mi sbaglio, anche PyTorch ha avuto una serie di contribution, OpenAI non mi suona tantissimo, a parmare potete anche sbagliarmi. Però comunque OpenAI è un'azienda, detto questo, stando per assunto che non hanno avuto contribution, in realtà appunto sono un po' bautarici, sono rifatti tutto quanto da casa loro, non sono inventati da qualunque, e quindi in realtà non hanno avuto bisogno di nessuno, da questo punto di vista. C'era anche Google, che si è fatta TensorFlow, e l'ha fatto all'inizio, poi ha fatto i suoi, poi ha accettato un sacco di contribuzioni open, tipo che ASP, per esempio, è stata inglobata poi. E quello era uno sforzo di community. Google ha beneficiato, dopo un po', però all'inizio ha fatto tutto da sé, i modelli si erano fatti da sé. Anche per dimostrare ai vari casi d'uso che TensorFlow era buono per farci l'addestramento, ha addestrato una serie di modelli con TensorFlow che fa vedere che funzionava e poi l'ha messo in modo "Zoo" in maniera tale che chiunque si possa scaricare un modello già testato. Non lo so, non ho una risposta particolarmente convincente, nel senso che è impossibile pensare di mettere la patente di guida nella community. Io c'ero quando piatavo gli mp3 su WinMX, non era semplicemente pensabile fermare la cosa. la cosa, era il FireWest e non era regolamentabile, non ci poteva fare nulla. L'unico modo in cui la situazione è rientrata è quando è uscito fuori Spotify, in cui hanno notato uno streaming di qualità togliendoci la pena di dover cercare la roba, se no ancora stiamo lì a scaricare gli mp3, no? Guarda io sono fermo nello stesso punto, nel senso che ho la consapevolezza che A) è uno strumento potente e B) servirebbe la patente per usarlo. Ma non la puoi imporre? Ma non puoi imporla o almeno non puoi imporla senza assorbirti tutti i controeffetti del proibizionismo più becero su uno strumento che è la rete che è difficilmente controllabile se non sei la Cina e anche se sei la Cina è difficilmente controllabile uguale. chiaro che o non facciamo niente oppure accettiamo di cedere una parte della nostra libertà perché andiamo a chiappare un punto nel quale non è possibile evitare di essere intercettati e questo punto è l'hardware banalmente se Amazon si compra tutte le GPU del mondo tu per addestrare una interna integrale devi passare da loro e allora a quel punto l'idolo scatta il controllo tu giri sul computer di qualcun altro e devi accettare che quel computer di qualcun altro ha dei termini e delle condizioni e a quel punto tu non puoi farne un uso scorretto anche perchè quel qualcun altro ci ha scritto nel suo Times and Conditions io ti posso nonire a controllare e a quel punto hai imposto l'ordine però devi imporre prima una strazzatura tale per cui la gente debba venire da te invece di farlo girare sul proprio computer tu non avrai altro computer al di fuori del mio mio cloud e tu devi venirci qui questo è l'unico modo in cui la vedo effettivamente la patente di guida serve solamente se vuoi andare nelle strade pubbliche le strade pubbliche sono in proprietà dello Stato lo Stato ti impone che se vuoi andare sulle strade pubbliche tu devi avere la patente. Se tu fossi eco e avessi un circuito di Formula 1 nel tuo giardino, la patente non serve, puoi ammazzarti nel tuo giardino come ti pare, no? Sì, sì. Allora tu devi, puoi fare rispettare la legge solo se la strada è di qualcun'altra, no? Se il computer è di qualcun'altra. Sì. È tutto lì. ed è quel qualcun altro il proprietario del computer che può farsi, che ha il diritto di far rispettare la legge, non la collettività. Esattamente, la collettività si allarga solo a un macchiatorio, non è che la collettività non si automodele, non si autoregola, non te lo devi aspettare. Sì, cambia però il concetto di stato, scambiamo il concetto di il concetto di corporation che non fittano. E poi a quel punto c'è una serie di corporation che si mettono tutte insieme, fai la alliance come è tanto di moda e a quel punto quelli fanno la tregua col governo e gli dicono ok noi rispettiamo questa carta dei valori e voi non ci venite a scocciare tutto il tempo se no lo stato comincia a svuotarli tutti i tanti giorni e è impossibile lavorare. Fantastico, anche su questo topic potremmo rimanere un'altra ora e 40, ma è arrivato il momento tipico e topico del nostro episodio. Il momento che noi chiamiamo "Paese dei Baluchi". Il momento nel quale sia i nostri host che i nostri guest condividono con noi e con la community un libro, un talk, un video, un paper, un videogioco, un disco, un vino... qualunque cosa vi ha catturato la loro attenzione pensano che sia importante scelare e condividere con la nostra community quindi alberto ti chiedo hai qualcosa da condividere con noi? ti vedo proprio adesso preoccupato eh perchè ho 250 idee che mi vengono in testa ma se hai più di una va bene uguale non ti preoccupare ok sparo la prima che mi viene a caso perchè effettivamente ci ho dedicato tanto tempo ultimamente Ragazzi andate sui vari Netflix e andate a vedere il film "The Dirt". "The Dirt" è un biopic, quindi è un film che parla della storia dei Motley Crue. E l'ho davvero apprezzato tanto, devo dire, rispetto ad altri biopic su rock'n'roll. Questo qua effettivamente l'ho apprezzato e come sono bassassesso lo stanno pure in turno, quindi me li vado a vedere. quindi in questo periodo mi sto studiando tutta la discografia dei gruppi per impararmi tutta la scaletta, perché quando vado al concerto io adoro cantare le canzoni a scorciagola e quindi praticamente tutto quello che è le loro canzoni, in questo momento tutto l'hard rock, seconda metà degli anni '80 della scena in Los Angeles, è quello che sta animando le mie giornate in questi giorni. Che bello! Mi hai dato un'idea già su cosa vedere il prossimo weekend? The Dirt Mappa Super, trovati la playlist del concerto che è fantastica! Stavo per continuare, è la playlist da usare domani mattina quando inizierò a lavorare? Io l'ho ricavata da Setlist.fm, che è il mio sito preferito per le playlist dei concerti, e proprio qualche brava persona su YouTube Music aveva già fatto la playlist del concerto, quindi a posto, ho visto che combaciavano le ordinazioni, a posto così, taaa, e me le sto mettendo a loop. Beh, belle cose, loro saranno un po' rottametti, però va bene così, in fin dei conti. Li hanno vissuti bene i loro anni, devo dire. Noi non c'eravamo, nel senso che eravamo appena nati quando loro stavano lì, che giovano grande quindi ci siamo un po' persi, un po' l'ultimo spiaggio a vederseli. Quindi andatevi poi andate su Amazon, su Discogs, sul sito che prendete e compratevi Girls Girls Girls, compratevi Dr. Feelgood, compratevi tutti questi bellissimi album che sono ore e ore di buono, di presa bene proprio, la musica per avere la presa bene, questi sono. Io invece, ah guarda, visto che stavo entrando nel mood c'ero quasi a condividere anche io un concerto, ma ve lo risparmio che è la prossima settimana. Oh su su, titillaci. Non lo so se ti piacciono. Che fai, ascoldi la mano. però ci sono i Kiss a Lione e volevo andarli a vedere ah, mi piacciono, li ho visti due volte anzi tre, ho visto tre volte i Kiss volevo andarli a vedere, quindi va bene, è uno spettacolo proprio di serie A i Kiss sono, penso, una delle band più scenografiche bravissimi, non sbagliano un colpo al vivo sono una goduria, assolutamente belli scenici anche se non sul mio completo ovviamente. Quella era insomma presa sulla scia di quello che dicevi, in realtà ho parlato con te qualche minuto fa di open source, free software, di quella differenza e di come sento quella differenza, non so chi di voi conosce Nadia Ekbal, è una docente, una ricercatrice, ha scritto un bellissimo libro che si intitola "Working in public the making and maintenance of open source software" è un libro carino da leggere, l'ha scritta qualche anno fa, non mi ricordo se l'ho già condiviso con voi ma c'è proprio tutto un capitolo che ragiona proprio su questa caratteristica del mondo dell'open source, quanto è cambiato nel tempo, quanta è stata l'influenza delle aziende e che cos'è oggi open source. Super interessante da leggere, io l'ho letto senza nessun pregiudizio per quello che era ieri e oggi l'open source, però è senza dubbio un'analisi che ha senso fermarsi a fare. 28 gradi e mezzo all'ombra, non piove più da 20 giorni, e non c'è un aleto di vento, caldo tanto caldo quanto assente, ja ja ja! Portaci da bere, uno shop di birra, presto portaci da bere, uno shop di birra, frutta con o senza schium, sugo a chiara ma che sia una birra! Una birra please! Oh yes sir! è il momento di ringraziare chi ci sostiene, che ci supporta, fa in modo che ogni settimana noi possiamo pagare le bollette dei servizi che utilizziamo per produrre il nostro podcast e quindi di riflesso possiamo arrivare alle vostre orecchie, nei vostri client di podcast. Questa settimana abbiamo due persone da ringraziare, uno è Giovanni Italiano, ciao Giovanni grazie, l'altra persona che si fatta carico di supportarci appunto in questo percorso è Livio Francesconi io vi ringrazio entrambi perché entrambi ci avete invitato tre birre, birre fondamentali per appunto fare in modo che il podcast possa arrivare alle orecchie di tutti così come lo conoscete. Tra l'altro Livio ci ha inviato un messaggio che dice Ciao ragazzi sto continuando ad ascoltare tutti i podcast all'indietro più i nuovi e sono al 97esimo approfitto per salutare tutta la community di Gitbar e anche la mia principale developer life salutando Mitch, gli amministratori e i membri Viva le communities io mi associo al saluto di Livio verso la community di Gitbar e verso la community di developer life [Musica] detto questo Alberto noi siamo arrivati alla parte terminale di questo episodio grazie mille per la bellissima chiacchierata guarda sono super felice perché per la prima volta con te siamo riusciti anche a sbirciare cosa c'è sotto il cofano come funzionano alcune cose sotto il cofano ed è una cosa che non si fa spesso spesso tutto il mondo delle AI vuoi perché è ormai diventata una buzzword AI, Machine Learning, chi più ne ha più ne metta viene trattato in modo un po' superficiale invece avere la possibilità con te di andare a capire alcuni dettagli tecnici come passare l'acqua nei tubi o come dare i ciaffoni in testa al modello con quelle metafore che l'hanno semplificato tantissimo è stato davvero fantastico. Quali sono le cose esoteiche per me, le cose che assolutamente non capisco mi erogno quasi a dirlo. La mia cosa esoteica è React, io non riesco a capire come si programmi in React, mi sembra una cosa esoteica, io so vecchio sono rimasto all'HTML generato per side, tipo già la roba con i template così per me React è un po'... Cioè aspetta, io non riesco a capire perché tu non riesci a capire React dopo quello che mi hai detto stasera Però l'ho fatto poi, due anni fa non ci ho capito una minchia, forse dovrei provarci, è stato difficile, è stato... sono riuscito ad andare oltre a tutt'un'ista e comunque l'idea che sparo sto HTML che poi HTML non è perché virtual DOM in realtà è il JSX ehm... no come di ogni otra sua sì, vero e questo evidenzia una cosa prima di chiudere voglio dirlo perché spesso mi scrivete in privato e mi dite "ma come fate a conoscere tutti questi topic? come fanno i tuoi ospiti a sapere così tante cose, che sanno tutto, sono dei gen... allora i nostri ospiti come Alberto ha evidenziato oggi sono dei specialisti in dei topic. Questo vuol dire che, io lo dico sempre, la conoscenza della persona non è... immaginate un foglio bianco, disegnate un punto al centro e tirate una circonferenza attorno. Tu vedi una persona che ne sa tantissimo ma la sua conoscenza non è una circonferenza ma è immaginate una circonferenza molto più piccola con degli spike quindi con delle aree molto più preeminenti. La mia che sono un junior per esempio immaginiamo che io sia un junior la mia che sono un è certo più più piccola rispetto a quella di Alberto ma io potrei avere degli spike in ambiti completamente diversi da quelli di Alberto e Gitbar nasce proprio per fare in modo che in qualche modo lo spike di Alberto condizioni anche la mia circonferenza generale io ne so un mezzo millimetro in più perché Alberto mi ha regalato parte dei suoi anni di studio e parte dei suoi anni di esperienza. Quel mezzo millimetro per quanto piccolo sia viene grazie appunto al Confront. E proprio per questo io ti ringrazio tantissimo Alberto. Grazie a te. Grazie di cuore. Io vi do appuntamento alla prossima puntata. Ciao ciao! GitBar, il circolo dei fullstack developer. Una volta a settimana ci troviamo davanti a due birre e con Brain Repo parliamo di linguaggi e tecniche di sviluppo web, di metodologie ed strumenti immancabili nella cassetta degli attrezzi dei Full Stack Dev.